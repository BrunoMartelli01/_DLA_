{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfe2f949",
   "metadata": {},
   "source": [
    "# Laboratory #4: Adversarial Learning and OOD Detection\n",
    "\n",
    "In this laboratory session we will develop a methodology for detecting OOD samples and measuring the quality of OOD detection. We will also experiment with incorporating adversarial examples during training to render models more robust to adversarial attacks.\n",
    "\n",
    "---\n",
    "## Exercise 1: OOD Detection and Performance Evaluation\n",
    "In this first exercise you will build a simple OOD detection pipeline and implement some performance metrics to evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce715ee",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Build a simple OOD detection pipeline\n",
    "\n",
    "Implement an OOD detection pipeline (like in the Flipped Activity notebook) using an ID and an OOD dataset of your choice. Some options:\n",
    "\n",
    "+ CIFAR-10 (ID), Subset of CIFAR-100 (OOD). You will need to wrap CIFAR-100 in some way to select a subset of classes that are *not* in CIFAR-10 (see `torch.utils.data.Subset`).\n",
    "+ Labeled Faces in the Wild (ID), CIFAR-10 or FakeData (OOD). The LfW dataset is available in Scikit-learn (see `sklearn.datasets.fetch_lfw_people`).\n",
    "+ Something else, but if using images keep the images reasonably small!\n",
    "\n",
    "In this exercise your *OOD Detector* should produce a score representing how \"out of distribution\" a test sample is. We will implement some metrics in the next exercise, but for now use the techniques from the flipped activity notebook to judge how well OOD scoring is working (i.e. histograms).\n",
    "\n",
    "\n",
    "**Note**: Make sure you make a validation split of your ID dataset for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3d997a3dccf9b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### NOTE: For this lab, some parts are already done, so I will not repeat them. I will only comment on my changes and additions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28c846cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T21:09:28.943144Z",
     "start_time": "2025-11-18T21:09:13.252145100Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import FakeData\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Subset\n",
    "from sklearn import metrics as skm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e220194-34f4-4d19-8ae7-e30ffca19dd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T21:09:28.974142700Z",
     "start_time": "2025-11-18T21:09:28.788143Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4858e0f-f946-421b-ab5f-a23c9df536f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T21:09:28.978143Z",
     "start_time": "2025-11-18T21:09:28.905142800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f9a87a5",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-11-18T21:09:42.541143500Z",
     "start_time": "2025-11-18T21:09:28.912147900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 88.4M/170M [00:11<00:10, 7.55MB/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 7\u001B[0m\n\u001B[0;32m      1\u001B[0m transform \u001B[38;5;241m=\u001B[39m transforms\u001B[38;5;241m.\u001B[39mCompose(\n\u001B[0;32m      2\u001B[0m     [transforms\u001B[38;5;241m.\u001B[39mToTensor(),\n\u001B[0;32m      3\u001B[0m      transforms\u001B[38;5;241m.\u001B[39mNormalize((\u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m0.5\u001B[39m), (\u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m0.5\u001B[39m))])\n\u001B[0;32m      5\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m128\u001B[39m\n\u001B[1;32m----> 7\u001B[0m trainset \u001B[38;5;241m=\u001B[39m \u001B[43mtorchvision\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdatasets\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCIFAR10\u001B[49m\u001B[43m(\u001B[49m\u001B[43mroot\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m./data\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43mdownload\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransform\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m trainloader \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataLoader(trainset, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, num_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m, persistent_workers\u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     11\u001B[0m testset \u001B[38;5;241m=\u001B[39m torchvision\u001B[38;5;241m.\u001B[39mdatasets\u001B[38;5;241m.\u001B[39mCIFAR10(root\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./data\u001B[39m\u001B[38;5;124m'\u001B[39m, train\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m     12\u001B[0m                                        download\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, transform\u001B[38;5;241m=\u001B[39mtransform)\n",
      "File \u001B[1;32m~\\PycharmProjects\\DLA\\.venv\\lib\\site-packages\\torchvision\\datasets\\cifar.py:66\u001B[0m, in \u001B[0;36mCIFAR10.__init__\u001B[1;34m(self, root, train, transform, target_transform, download)\u001B[0m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain \u001B[38;5;241m=\u001B[39m train  \u001B[38;5;66;03m# training set or test set\u001B[39;00m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m download:\n\u001B[1;32m---> 66\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_integrity():\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\DLA\\.venv\\lib\\site-packages\\torchvision\\datasets\\cifar.py:139\u001B[0m, in \u001B[0;36mCIFAR10.download\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_integrity():\n\u001B[0;32m    138\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m--> 139\u001B[0m \u001B[43mdownload_and_extract_archive\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mroot\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmd5\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtgz_md5\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\DLA\\.venv\\lib\\site-packages\\torchvision\\datasets\\utils.py:391\u001B[0m, in \u001B[0;36mdownload_and_extract_archive\u001B[1;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001B[0m\n\u001B[0;32m    388\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m filename:\n\u001B[0;32m    389\u001B[0m     filename \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mbasename(url)\n\u001B[1;32m--> 391\u001B[0m \u001B[43mdownload_url\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdownload_root\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmd5\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    393\u001B[0m archive \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(download_root, filename)\n\u001B[0;32m    394\u001B[0m extract_archive(archive, extract_root, remove_finished)\n",
      "File \u001B[1;32m~\\PycharmProjects\\DLA\\.venv\\lib\\site-packages\\torchvision\\datasets\\utils.py:130\u001B[0m, in \u001B[0;36mdownload_url\u001B[1;34m(url, root, filename, md5, max_redirect_hops)\u001B[0m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;66;03m# download the file\u001B[39;00m\n\u001B[0;32m    129\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 130\u001B[0m     \u001B[43m_urlretrieve\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (urllib\u001B[38;5;241m.\u001B[39merror\u001B[38;5;241m.\u001B[39mURLError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[0;32m    132\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m url[:\u001B[38;5;241m5\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\DLA\\.venv\\lib\\site-packages\\torchvision\\datasets\\utils.py:30\u001B[0m, in \u001B[0;36m_urlretrieve\u001B[1;34m(url, filename, chunk_size)\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m urllib\u001B[38;5;241m.\u001B[39mrequest\u001B[38;5;241m.\u001B[39murlopen(urllib\u001B[38;5;241m.\u001B[39mrequest\u001B[38;5;241m.\u001B[39mRequest(url, headers\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUser-Agent\u001B[39m\u001B[38;5;124m\"\u001B[39m: USER_AGENT})) \u001B[38;5;28;01mas\u001B[39;00m response:\n\u001B[0;32m     29\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(filename, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m fh, tqdm(total\u001B[38;5;241m=\u001B[39mresponse\u001B[38;5;241m.\u001B[39mlength, unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mB\u001B[39m\u001B[38;5;124m\"\u001B[39m, unit_scale\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[1;32m---> 30\u001B[0m         \u001B[38;5;28;01mwhile\u001B[39;00m chunk \u001B[38;5;241m:=\u001B[39m \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m     31\u001B[0m             fh\u001B[38;5;241m.\u001B[39mwrite(chunk)\n\u001B[0;32m     32\u001B[0m             pbar\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;28mlen\u001B[39m(chunk))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:466\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[1;34m(self, amt)\u001B[0m\n\u001B[0;32m    463\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m amt \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength:\n\u001B[0;32m    464\u001B[0m     \u001B[38;5;66;03m# clip the read to the \"end of response\"\u001B[39;00m\n\u001B[0;32m    465\u001B[0m     amt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength\n\u001B[1;32m--> 466\u001B[0m s \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    467\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m s \u001B[38;5;129;01mand\u001B[39;00m amt:\n\u001B[0;32m    468\u001B[0m     \u001B[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001B[39;00m\n\u001B[0;32m    469\u001B[0m     \u001B[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001B[39;00m\n\u001B[0;32m    470\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_conn()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:705\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[1;34m(self, b)\u001B[0m\n\u001B[0;32m    703\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    704\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 705\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    706\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[0;32m    707\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1274\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[1;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[0;32m   1270\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1271\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1272\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[0;32m   1273\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[1;32m-> 1274\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1275\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1276\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1130\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[1;34m(self, len, buffer)\u001B[0m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1129\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1130\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1131\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1132\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=8, persistent_workers= True)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=8, persistent_workers= True)\n",
    "\n",
    "fakeset = FakeData(size=len(testset), image_size=(3, 32, 32), transform=transform)\n",
    "fakeloader = torch.utils.data.DataLoader(fakeset, batch_size=batch_size, shuffle=False, num_workers=8, persistent_workers= True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a91a2613c41a436",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Goal\n",
    "To split a single, existing dataset into two separate, non-overlapping sets(one for training (90%) and one for validation (10%)) and then to create data loaders for both.\n",
    "\n",
    "#### Method\n",
    "1.  **Calculate Split Sizes**: The code first determines the exact number of samples that will go into the training set (90% of the total) and the validation set (the remaining 10%).\n",
    "\n",
    "2.  **Define Indices**: It then creates two simple lists of indices. `train_indices` contains the indices for the first 90% of the data, and `val_indices` contains the indices for the last 10%.\n",
    "\n",
    " 3. **Create Subset objects**: The code creates `Subset` objects by passing the `trainset` and the selected `indices`. A `Subset` is a lightweight wrapper that keeps a reference to the original `trainset` and stores only the `indices`, so the data is never duplicated. This makes it highly memory-efficient\n",
    "4.  **Create `DataLoader`s**: Finally, two `DataLoader` objects are created from these new `Subset`s. These loaders will handle the process of fetching data in batches for the training loop.\n",
    "    *   **`trainloader`**: Has `shuffle=True`. This is important for training, as it ensures the model sees the data in a different random order each epoch, which helps it generalize better.\n",
    "    *   **`valloader`**: Has `shuffle=False`. This is a best practice for evaluation. The validation data should always be presented in the same order so that you can get consistent, comparable performance metrics from one epoch to the next.\n",
    "    *   `num_workers` and `persistent_workers` are performance optimizations that use multiple CPU processes to load data in the background, preventing data loading from becoming a bottleneck during training.\n",
    "\n",
    "#### Result\n",
    "The code produces two `DataLoader` objects:\n",
    "*   `trainloader`: Ready to be used in the training loop, it will serve shuffled batches of data drawn from the first 90% of the original dataset.\n",
    "*   `valloiloader`: Ready to be used in the evaluation loop, it will serve unshuffled batches of data drawn from the last 10% of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e126a1398582d70e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-18T21:09:42.656143200Z",
     "start_time": "2025-11-18T21:09:42.543144400Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_size = int(0.9 * len(trainset))  # 90% train\n",
    "val_size = len(trainset) - train_size  # 10% validation\n",
    "\n",
    "train_indices = list(range(0, train_size))\n",
    "val_indices = list(range(train_size, len(trainset)))\n",
    "train_subset = Subset(trainset, train_indices)\n",
    "val_subset = Subset(trainset, val_indices)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=8, persistent_workers=True)\n",
    "\n",
    "valloader = torch.utils.data.DataLoader(val_subset, batch_size=batch_size,\n",
    "                                        shuffle=False, num_workers=8, persistent_workers=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffb6a75-cbe8-4b0d-a9a5-008b7bf6f968",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.544143500Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1)  # downsample\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)  # downsample\n",
    "        \n",
    "        self.flatten_dim = 256 * 8 * 8  # assuming input is 32x32\n",
    "        self.fc1 = nn.Linear(self.flatten_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))   # -> 32x32x32\n",
    "        x = F.relu(self.conv2(x))   # -> 32x32x64\n",
    "        x = F.relu(self.conv3(x))   # -> 32x32x128\n",
    "        x = F.relu(self.conv4(x))   # -> 16x16x128\n",
    "        x = F.relu(self.conv5(x))   # -> 8x8x256\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6748ff937a2df607",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Goal\n",
    "To train the CNN model while monitoring its performance on a validation set, and to save the model weights from the epoch that achieves the lowest validation loss. This ensures we end up with the best possible version of our model, preventing overfitting.\n",
    "\n",
    "#### Method\n",
    "\n",
    "1.  **Initialization**: Before we call the function, several key components are set up:\n",
    "    *   **Hyperparameters**: The number of `epochs`, the `loss_fn` (Cross-Entropy Loss for classification), and the `optimizer` (AdamW) are defined.\n",
    "    *   **Inside the train**: Variables like `best_val_loss` (initialized to infinity), `best_model_wts`, and `path` are created to keep track of the best model found so far and where to save it.\n",
    "\n",
    "2.  **Outer Loop (Epochs)**: The main `for` loop iterates through the specified number of epochs, with a `tqdm` progress bar to provide a visual of the overall training progress.\n",
    "\n",
    "3.  **Training Phase (per epoch)**:\n",
    "    *   The code loops through the `trainloader`. For each batch of data, it performs the standard training steps:\n",
    "        1.  Move data to the correct `device` (GPU).\n",
    "        2.  `optimizer.zero_grad()`: Clear any gradients from the previous step.\n",
    "        3.  `yp = model(x)`: Perform a forward pass to get the model's predictions.\n",
    "        4.  `l = loss_fn(yp, y)`: Calculate the loss between the predictions and the true labels.\n",
    "        5.  `l.backward()`: Perform backpropagation to compute the gradients.\n",
    "        6.  `optimizer.step()`: Update the model's weights using the computed gradients.\n",
    "    *   The `train_loss` for the epoch is calculated as the average of the batch losses.\n",
    "\n",
    "4.  **Validation Phase (per epoch)**:\n",
    "    *   `with torch.no_grad()`: This context manager disables gradient calculations. It reduces memory consumption and speeds up the process, as we are not learning in this phase.\n",
    "    *   The code loops through the `valloader`, calculating the `val_loss` and the `val_acc` (validation accuracy) over all batches. No `optimizer.step()` is called here.\n",
    "\n",
    "5.  **Best Model Checkpointing**:\n",
    "    *   After each epoch, the current `val_loss` is compared to the `best_val_loss` found so far.\n",
    "    *   If the current loss is lower, it means we've found a new best model. The script then:\n",
    "        1.  Updates `best_val_loss` and records the current `best_epoch`.\n",
    "        2.  Saves a copy of the model's current weights (`model.state_dict()`) to the specified `path`. This is the **checkpointing** step.\n",
    "\n",
    "6.  **Logging and Final Loading**:\n",
    "    *   The `tqdm` progress bar is updated with all the relevant metrics (`train_loss`, `val_loss`, `val_acc`, and the best scores so far).\n",
    "    *   After the entire training loop finishes, the script loads the saved best weights back into the `model` object. This ensures that the model available for use after training is the best-performing one, not just the one from the final epoch.\n",
    "#### Result\n",
    "*   **A Saved Checkpoint**: A file named `cifar10_best_P.pth` is created on disk. This file contains the weights of the model at the epoch where it achieved the lowest validation loss.\n",
    "*   **A Trained Model**: The `model` object in the Python session is loaded with these best weights, making it immediately ready for inference on a test set.\n",
    "*   **Log**: The console output provides a clear, epoch-by-epoch log of the training process, showing how the model's performance on both the training and validation sets evolved over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d197bb0e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.545142Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#1e-5 001\n",
    "def train(model, trainloader, valloader, loss_fn, optimizer, epochs, path):\n",
    "        \n",
    "    best_val_loss = float(\"inf\") \n",
    "    best_model_wts = None\n",
    "    best_epoch = 0  \n",
    "    acc_of_best = 0\n",
    "    \n",
    "    epoch_pbar = tqdm(range(epochs), desc=\"Training Progress\")\n",
    "    \n",
    "    for e in epoch_pbar:\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for data in trainloader:\n",
    "            x, y = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            yp = model(x)\n",
    "            l = loss_fn(yp, y)\n",
    "    \n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += l.item()\n",
    "        train_loss = running_loss / len(trainloader)\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in valloader:\n",
    "                x, y = data[0].to(device), data[1].to(device)\n",
    "                yp = model(x)\n",
    "                l = loss_fn(yp, y)\n",
    "                _, predicted = yp.max(1)\n",
    "                val_loss += l.item()\n",
    "                \n",
    "                total += y.size(0)\n",
    "                correct += predicted.eq(y).sum().item()\n",
    "        \n",
    "        #    scheduler.step()\n",
    "        val_loss /= len(valloader)\n",
    "        val_acc = 100. * correct / total\n",
    "        \n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = e + 1 \n",
    "            best_model_wts = model.state_dict().copy()\n",
    "            torch.save(best_model_wts, path)\n",
    "            acc_of_best = val_acc\n",
    "        \n",
    "        metrics = {\n",
    "            'train_loss': f\"{train_loss:.4f}\", \n",
    "            'val_loss': f\"{val_loss:.4f}\", \n",
    "            'val_acc': f\"{val_acc:.2f}%\",\n",
    "        }\n",
    "        \n",
    "        if best_epoch > 0:\n",
    "            metrics['best_loss'] = f\"{best_val_loss:.4f}\"\n",
    "            metrics['best_epoch'] = best_epoch\n",
    "            metrics['best_acc'] = f\"{acc_of_best:.2f}%\" \n",
    "        epoch_pbar.set_postfix(metrics)\n",
    "\n",
    "   \n",
    "    if best_model_wts is not None:\n",
    "            model.load_state_dict(best_model_wts)\n",
    "            print(f\"\\nBest model from epoch {best_epoch} loaded (Val Loss: {best_val_loss:.4f}).\")\n",
    "    return model\n",
    "        \n",
    "\n",
    "\n",
    "model = CNN().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#epochs = 50\n",
    "#optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "epochs = 100\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay = 5e-4)\n",
    "#scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0)\n",
    "path =  './cifar10_best_P.pth'\n",
    "base_loss = nn.CrossEntropyLoss()\n",
    "model = train(model, trainloader, valloader, loss_fn, optimizer, epochs, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c1fd14",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.546142700Z"
    }
   },
   "outputs": [],
   "source": [
    "model = CNN().to(device)\n",
    "model.load_state_dict(torch.load(path))\n",
    "model.eval()\n",
    "y_gt, y_pred = [], []\n",
    "for it, data in enumerate (testloader):\n",
    "    x, y = data\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    yp = model(x)\n",
    "    y_pred.append(yp.argmax(1))\n",
    "    y_gt.append(y)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1bc79f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.547145200Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_t = torch.cat(y_pred)\n",
    "y_gt_t = torch.cat(y_gt)\n",
    "\n",
    "accuracy = sum(y_pred_t == y_gt_t)/len(y_gt_t)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "cmn = skm.confusion_matrix(y_gt_t.cpu(), y_pred_t.cpu(), normalize='true')\n",
    "cmn = (100*cmn).astype(np.int32)\n",
    "disp = metrics.ConfusionMatrixDisplay(cmn, display_labels=testset.classes)\n",
    "disp.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e86784950dc6cf4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Goal\n",
    "To test the effectiveness of using the maximum logit score as a simple confidence metric to distinguish between in-distribution (`testloader`) and out-of-distribution (`fakeloader`) data. This will serve as the initial benchmark to improve upon.\n",
    "\n",
    "#### Method\n",
    "1.  **The Scoring Function (`compute_scores`)**: A function is defined to calculate a confidence score for each image in a dataset.\n",
    "    *   It takes a trained `model` and a `data_loader` as input.\n",
    "    *   Inside the function, it iterates through the data and passes each batch through the model to get the raw output logits.\n",
    "    *   For each image, the score is calculated as `logit.max(dim=1)[0]`. This takes the **maximum value from the logit vector**. This maximum logit corresponds to the class the model thinks is most likely. In the literature, this is known as the Maximum Softmax Probability (MSP) method, since the `softmax` function preserves the order of the logits, and a higher maximum logit directly translates to a higher confidence probability for the predicted class.\n",
    "\n",
    "2.  **Applying the Score Function**:\n",
    "    *   The best-performing model (saved previously at `path`) is loaded.\n",
    "    *   The `compute_scores` function is called twice:\n",
    "        *   Once on the `testloader` to get the confidence scores for **in-distribution (ID)** data.\n",
    "        *   Once on the `fakeloader` to get the confidence scores for **out-of-distribution (OOD)** data.\n",
    "\n",
    "3.  **Visualization**:\n",
    "    *   The two sets of scores (`scores_test` and `scores_fake`) are sorted independently.\n",
    "    *   They are then plotted on the same graph. This allows for a direct visual comparison of the entire distribution of confidence scores for ID vs. OOD data.\n",
    "\n",
    "#### Result\n",
    "*   The output is a plot containing two curves. The x-axis can be seen as the percentile of the data, and the y-axis is the confidence score.\n",
    "*   **Ideal Outcome**: If this method works well, the blue curve (`test`) should be consistently **above** the orange curve (`fake`). This would visually confirm that the model is indeed more confident on in-distribution data than on out-of-distribution data.\n",
    "*   **The Baseline Reality**: In practice, while there will likely be a separation, the two curves will also have a significant **overlap**. This overlap represents the failure region where this simple detector cannot distinguish between ID and OOD samples based on confidence alone. This imperfect separation is why this method is considered a **baseline**. The rest of the lab will introduce more advanced techniques designed to increase the separation between these two curves and create a more reliable OOD detector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b84f64-51de-4577-aa0f-512cfbc3aba0",
   "metadata": {},
   "source": [
    "**NOTE**: From now on, I’ll use my trained CNN. If a different one is needed, change the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0f84a2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.548144700Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_scores(model, data_loader,device, **kwargs):\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            x, y = data\n",
    "            logit = model(x.to(device))\n",
    "            score = logit.max(dim=1)[0]\n",
    "            scores.append(score)\n",
    "        scores = torch.cat(scores)\n",
    "        return scores\n",
    "path= './cifar10_best_P.pth'\n",
    "model.load_state_dict(torch.load(path)) \n",
    "scores_test = compute_scores(model, testloader, device)\n",
    "scores_fake = compute_scores(model, fakeloader, device)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "\n",
    "axes[0].plot(sorted(scores_test.cpu()), label='test', color='C0')\n",
    "axes[0].plot(sorted(scores_fake.cpu()), label='fake', color='C1')\n",
    "axes[0].set_title(\"Sorted Score Curves\")\n",
    "axes[0].set_xlabel(\"Samples (sorted index)\")\n",
    "axes[0].set_ylabel(\"Score\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(scores_test.cpu(), density=True, alpha=0.5, bins=25, label='test', color='C0')\n",
    "axes[1].hist(scores_fake.cpu(), density=True, alpha=0.5, bins=25, label='fake', color='C1')\n",
    "axes[1].set_title(\"Score Distributions (Histogram)\")\n",
    "axes[1].set_xlabel(\"Score\")\n",
    "axes[1].set_ylabel(\"Density\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21691aa8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.549142600Z"
    }
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Input size: [batch, 3, 32, 32]\n",
    "        # Output size: [batch, 3, 32, 32]\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 12, 4, stride=2, padding=1),            # [batch, 12, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(12, 24, 4, stride=2, padding=1),           # [batch, 24, 8, 8]\n",
    "            nn.ReLU(),\n",
    "\t\t\tnn.Conv2d(24, 48, 4, stride=2, padding=1),           # [batch, 48, 4, 4]\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "\t\t\tnn.ConvTranspose2d(48, 24, 4, stride=2, padding=1),  # [batch, 24, 8, 8]\n",
    "            nn.ReLU(),\n",
    "\t\t\tnn.ConvTranspose2d(24, 12, 4, stride=2, padding=1),  # [batch, 12, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(12, 3, 4, stride=2, padding=1),   # [batch, 3, 32, 32]\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c0a9412bac1b9b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Goal\n",
    "To train an Autoencoder to accurately reconstruct images from the in-distribution training set (CIFAR-10). The objective is to save the model that achieves the lowest reconstruction error on the validation set, creating a specialist model whose reconstruction quality can be used to detect anomalies.\n",
    "\n",
    "#### Method\n",
    "1.  **Model and Loss Function**: An `Autoencoder` model is initialized. The chosen loss function is `nn.MSELoss` (Mean Squared Error). This is the critical difference from a classification task. Instead of comparing model outputs to class labels, MSE directly measures the average squared difference between the pixel values of the original input image (`x`) and the reconstructed output image (`x_rec`). The optimizer's goal is to minimize this pixel-wise error.\n",
    "\n",
    "2.  **Training Loop**: The script iterates for a set number of epochs. In each epoch:\n",
    "    *   **Training Phase**: The model processes batches from the `trainloader`. For each batch, it performs a forward pass to get the reconstructed image (`x_rec`), calculates the `MSELoss` between the original and the reconstruction, and uses backpropagation to update the model weights.\n",
    "    *   **Validation Phase**: The model then evaluates its performance on the `valloader`. It calculates the average reconstruction loss on this unseen portion of the in-distribution data.\n",
    "\n",
    "3.  **Checkpointing**: Just like in the classifier training, the script keeps track of the `best_loss` on the validation set. If the model achieves a new lowest validation loss in the current epoch, it saves the model's weights (`state_dict`) to disk. This ensures the final model is the one that generalizes best to unseen ID data.\n",
    "#### Result\n",
    "*   The script produces a trained `Autoencoder` model, with the best-performing weights saved to `cifar10_ae.pth`.\n",
    "*   This model is now a specialist, highly optimized to compress and decompress images from the CIFAR-10 dataset with minimal reconstruction error.\n",
    "*   It's interesting to note that the autoencoder did not overfit. This is likely due to the fact that the **information bottleneck** successfully regularizes the model, and the task itself is more complex, changing from **classification to pixel-wise reconstruction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e92e7054edce18",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.550145700Z"
    }
   },
   "outputs": [],
   "source": [
    "model_ae = Autoencoder().to(device)\n",
    "mse_loss = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_ae.parameters(), lr=0.001)\n",
    "epochs = 100\n",
    "best_loss = float('inf')\n",
    "path = './cifar10_ae.pth'\n",
    "val_loss = 0\n",
    "epoch = 0 \n",
    "ae_bar = tqdm(range(epochs), desc=\"AE Training Progress\")\n",
    "\n",
    "for e in ae_bar:\n",
    "    running_loss = 0\n",
    "    for data in trainloader:\n",
    "        x, y = data\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        z, x_rec = model_ae(x)\n",
    "        l = mse_loss(x, x_rec)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += l.item()\n",
    "    val_loss = 0\n",
    "    \n",
    "    for data in valloader:\n",
    "        x, y = data\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        z, x_rec = model_ae(x)\n",
    "        l = mse_loss(x, x_rec)\n",
    "        val_loss += l.item()\n",
    "    val_loss /= len(valloader)\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model_ae.state_dict(), path)\n",
    "        epoch = e \n",
    "    ae_bar.set_postfix({'train_loss': f\"{running_loss/len(trainloader):.4f}\", 'val_loss': f\"{val_loss:.4f}\", 'best_loss': f\"{best_loss:.4f}\", 'best_epoch': f\"{epoch}\"}, )\n",
    "model_ae.load_state_dict(torch.load(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867db6ab",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.551145100Z"
    }
   },
   "outputs": [],
   "source": [
    "path = './cifar10_ae.pth'\n",
    "loss = nn.MSELoss(reduction='none') \n",
    "model_ae = Autoencoder().to(device)\n",
    "model_ae.load_state_dict(torch.load(path))\n",
    "\n",
    "def compute_scores_ae(model_ae, data_loader, device, **kwargs):\n",
    "    model_ae.eval()\n",
    "    scores_fake_ae = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            x, y = data\n",
    "            x=x.to(device)\n",
    "            z, xr = model_ae(x)\n",
    "            l = loss(x, xr)\n",
    "            score = l.mean([1,2,3])\n",
    "            scores_fake_ae.append(-score)\n",
    "    return torch.cat(scores_fake_ae)\n",
    "\n",
    "scores_fake_ae = compute_scores_ae(model_ae, fakeloader, device)\n",
    "scores_test_ae = compute_scores_ae(model_ae, testloader, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8815d41d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.553144300Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "\n",
    "axes[0].plot(sorted(scores_test_ae.cpu()), label='test', color='C0')\n",
    "axes[0].plot(sorted(scores_fake_ae.cpu()), label='fake', color='C1')\n",
    "axes[0].set_title(\"Sorted Score Curves\")\n",
    "axes[0].set_xlabel(\"Samples (sorted index)\")\n",
    "axes[0].set_ylabel(\"Score\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(scores_test_ae.cpu(), density=True, alpha=0.5, bins=25, label='test', color='C0')\n",
    "axes[1].hist(scores_fake_ae.cpu(), density=True, alpha=0.5, bins=25, label='fake', color='C1')\n",
    "axes[1].set_title(\"Score Distributions (Histogram)\")\n",
    "axes[1].set_xlabel(\"Score\")\n",
    "axes[1].set_ylabel(\"Density\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23bb962",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Measure your OOD detection performance\n",
    "\n",
    "There are several metrics used to evaluate OOD detection performance, we will concentrate on two threshold-free approaches: the area under the Receiver Operator Characteristic (ROC) curve for ID classification, and the area under the Precision-Recall curve for *both* ID and OOD scoring. See [the ODIN paper](https://arxiv.org/pdf/1706.02690.pdf) section 4.3 for a description of OOD metrics.\n",
    "\n",
    "Use the functions in `sklearn.metrics` to produce ROC and PR curves for your OOD detector. Some useful functions:\n",
    "\n",
    "+ [`sklearn.metric.RocCurveDisplay.from_predictions`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html)\n",
    "+ [`sklearn.metrics.PrecisionRecallDisplay`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.PrecisionRecallDisplay.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee7091ba8db74a4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Goal\n",
    "To create a single, reusable function that calculates a suite of standard OOD performance metrics: **AUROC**, **AUPR**, and **FPR at 95% TPR**. These metrics are taken from **[the ODIN paper](https://arxiv.org/pdf/1706.02690.pdf)** and provide a comprehensive way to measure how well a given scoring method can separate ID and OOD data.\n",
    "\n",
    "#### Method\n",
    "\n",
    "The function's methodology is based on reframing the OOD detection problem as a **binary classification task**:\n",
    "\n",
    "1.  **Score Calculation**: It first calls the provided `score_fn` (e.g., `compute_scores`) to get a single numerical score for every sample in both the ID and OOD data loaders.\n",
    "\n",
    "2.  **Ground Truth Setup**: It then creates the ground truth labels (`y_true`). By convention, in-distribution samples are the \"positive\" class, so they are assigned a label of `1`. Out-of-distribution samples are the \"negative\" class, assigned a label of `0`.\n",
    "\n",
    "3.  **Metric Calculation**: With the scores (`y_score`) and true labels (`y_true`), it uses `sklearn.metrics` to compute several key metrics:\n",
    "    *   **AUROC (Area Under the Receiver Operating Characteristic Curve)**:  It measures the overall ability of the score to separate the two distributions. An AUROC of 1.0 means perfect separation (all ID scores are higher than all OOD scores), while 0.5 means the score is no better than random chance.\n",
    "    *   **AUPR (Area Under the Precision-Recall Curve)**: Similar to AUROC, this metric also summarizes the detector's performance across all thresholds. It is particularly informative when there is a large imbalance between the number of ID and OOD samples. A score of 1.0 is perfect.\n",
    "    *   **FPR@95TPR (False Positive Rate at 95% True Positive Rate)**: measures how many out-of-distribution samples are mistakenly accepted when the detector is tuned to correctly identify 95% of in-distribution samples. A lower value means better performance.\n",
    "    *   **Detection Error**: This is another point-based metric, calculated at the same 95% TPR threshold. It's the average of the two types of errors at that threshold: the false positive rate (`fpr_at_95`) and the false negative rate (`1 - tpr`). A lower value is better.\n",
    "\n",
    "#### Result\n",
    "The function returns a **dictionary** containing all the calculated metrics for a given OOD detection experiment. This dictionary can be easily logged, compared across different scoring methods, or used for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95206dc54d2708bd",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.553144300Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def eval_OOD(model, idloader, oddloader, device,  score_fn=compute_scores, T=100, eps= 0.01):\n",
    "    id_scores = score_fn(model, idloader, device, T=T, eps= eps, filter_correct_only= True ).cpu()\n",
    "    ood_scores = score_fn(model, oddloader,  device, T=T, eps= eps, filter_correct_only= False ).cpu().numpy()\n",
    "    num_samples = min(len(id_scores), len(ood_scores))\n",
    "    id_scores = id_scores[:num_samples]\n",
    "    \n",
    "    \n",
    "    ood_scores = ood_scores[:num_samples]\n",
    "    y_true = np.concatenate([np.ones_like(id_scores), np.zeros_like(ood_scores)], axis=0)\n",
    "    y_score = np.concatenate([id_scores, ood_scores], axis=0)\n",
    "    label = \"FakeDataset-OOD\"\n",
    "    \n",
    "    fpr, tpr, thresholds = skm.roc_curve(y_true, y_score)\n",
    "    auroc = skm.auc(fpr, tpr)\n",
    "    precision, recall, _ = skm.precision_recall_curve(y_true, y_score)\n",
    "    aupr = skm.auc(recall, precision)\n",
    "    \n",
    "    \n",
    "    target_tpr = 0.95\n",
    "    idx = (torch.tensor(tpr) - target_tpr).abs().argmin().item()\n",
    "    fpr_at_95 = float(fpr[idx])\n",
    "    detection_error = 0.5 * (1 - target_tpr) + 0.5 * fpr_at_95\n",
    "    \n",
    "   \n",
    "    \n",
    "    return {\n",
    "    \"AUROC\": auroc,\n",
    "    \"AUPR\": aupr,\n",
    "    \"FPR@95TPR\": fpr_at_95,\n",
    "    \"DetectionError@95TPR\": detection_error,\n",
    "    \"roc_curve\": (fpr, tpr),\n",
    "    \"pr_curve\": (recall, precision),\n",
    "    \"label\": label\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f8674378a9d798",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.555144800Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_curve(metrics_dict, title_prefix=\"\",):\n",
    "    auc_digits = 4\n",
    "    fpr, tpr = metrics_dict[\"roc_curve\"]\n",
    "    recall, precision = metrics_dict[\"pr_curve\"]\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    disp = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=metrics_dict[\"AUROC\"])\n",
    "    disp.plot(ax=plt.gca())\n",
    "    plt.title(f\"{title_prefix} ROC Curve\")\n",
    "    \n",
    "    new_label = [ f\"AUROC ={metrics_dict['AUROC']:.{auc_digits}f}\" ]\n",
    "    plt.legend(new_label, loc=\"lower right\")\n",
    "    \n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    disp_pr = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
    "    disp_pr.plot(ax=plt.gca())\n",
    "    plt.title(f\"{title_prefix} Precision-Recall Curve\")\n",
    "    \n",
    "    new_label = [ f\"AUPR ={metrics_dict['AUPR']:.{auc_digits}f}\" ]\n",
    "    plt.legend( new_label, loc=\"lower left\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25c265efbdbecf0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.557145Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'./cifar10_best_P.pth'))\n",
    "#model.load_state_dict(torch.load(path))\n",
    "metric = eval_OOD(model, testloader, fakeloader, device, compute_scores)\n",
    "print(f\"  FPR at 95 TPR: {metric['FPR@95TPR']:.4f} | Detection Error at 95 tpr: {metric['DetectionError@95TPR']:.4f}\")\n",
    "plot_curve(metric, title_prefix=\"CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2b7146",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.558144300Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "metric = eval_OOD(model_ae, testloader, fakeloader,device, compute_scores_ae)\n",
    "print(f\"  FPR at 95 TPR: {metric['FPR@95TPR']:.4f} | Detection Error at 95 tpr: {metric['DetectionError@95TPR']:.4f}\")\n",
    "plot_curve(metric, title_prefix=\"Autoencoder\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30274dfaf32277e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "When comparing the CNN and the Autoencoder for out-of-distribution detection, their behaviors differ in important ways. The CNN delivers more stable performance, with a smoother and more consistent detection at 95 TPR. The Autoencoder, on the other hand, achieves higher overall discriminative power but concentrates its errors in a narrow region. This means it performs extremely well when operating in a conservative regime, but its performance drops sharply if the system requires very high TPR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7dcaa0",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 2: Enhancing Robustness to Adversarial Attack\n",
    "\n",
    "In this second exercise we will experiment with enhancing our base model to be (more) robust to adversarial attacks. \n",
    "\n",
    "### Exercise 2.1: Implement FGSM and generate adversarial examples\n",
    "\n",
    "Recall that the Fast Gradient Sign Method (FGSM) perturbs samples in the direction of the gradient with respect to the input $\\mathbf{x}$:\n",
    "$$ \\boldsymbol{\\eta}(\\mathbf{x}) = \\varepsilon \\mathrm{sign}(\\nabla_{\\mathbf{x}} \\mathcal{L}(\\boldsymbol{\\theta}, \\mathbf{x}, y)) ) $$\n",
    "Implement FGSM and generate some *adversarial examples* using your trained ID model. Evaluate these samples qualitatively and quantitatively. Evaluate how dependent on $\\varepsilon$ the quality of these samples are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804c69d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.559146Z"
    }
   },
   "outputs": [],
   "source": [
    "class NormalizeInverse(torchvision.transforms.Normalize):\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        mean = torch.as_tensor(mean)\n",
    "        std = torch.as_tensor(std)\n",
    "        std_inv = 1 / (std + 1e-7)\n",
    "        mean_inv = -mean * std_inv\n",
    "        super().__init__(mean=mean_inv, std=std_inv)\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return super().__call__(tensor.clone())\n",
    "\n",
    "inv = NormalizeInverse((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1aca71-23f6-4388-bc6f-c2e072dc4557",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.560143700Z"
    }
   },
   "outputs": [],
   "source": [
    "for i,c in enumerate(testset.classes):\n",
    "    print(i, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd2293b",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.578144400Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def fgsm_attack(model, testloader, device, sample_id=0, eps=1/255, targeted_attack=True, target_label=None):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "\n",
    "    for data in testloader:\n",
    "         x, y = data\n",
    "        \n",
    "\n",
    "         x, y = x[sample_id].to(device), y[sample_id].to(device)\n",
    "         x = x.unsqueeze(0)\n",
    "         y = y.unsqueeze(0)\n",
    "         x.requires_grad = True\n",
    "         original = x.clone()\n",
    "         output = model(x)\n",
    "         if output.argmax().item() == y.item() or (target_label is not None and y.item() != target_label):\n",
    "            break\n",
    "         else:\n",
    "            print('Classifier is already wrong or target label matches ground truth.')\n",
    "             \n",
    "    print('Attack!')\n",
    "    n = 0\n",
    "    target = torch.tensor(target_label).unsqueeze(0).to(device) if targeted_attack and target_label is not None else y\n",
    "\n",
    "    while True:\n",
    "        x.retain_grad()\n",
    "        output = model(x)\n",
    "        model.zero_grad()\n",
    "\n",
    "        loss = loss_fn(output, target if targeted_attack else y)\n",
    "        loss.backward()\n",
    "\n",
    "        if targeted_attack:\n",
    "            x = x - eps * torch.sign(x.grad)\n",
    "        else:\n",
    "            x = x + eps * torch.sign(x.grad)\n",
    "\n",
    "        n += 1\n",
    "        pred = output.argmax().item()\n",
    "\n",
    "        if not targeted_attack and pred != y.item():\n",
    "            print(f'Untargeted attack success! budget: {int(255 * n * eps)}/255')\n",
    "            break\n",
    "\n",
    "        if targeted_attack and pred == target.item():\n",
    "            print(f'Targeted attack ({pred}) success! budget: {int(255 * n * eps)}/255')\n",
    "            break\n",
    "\n",
    "    return original, x, y, output\n",
    "\n",
    "\n",
    "# MAIN\n",
    "if __name__ == \"__main__\":\n",
    "    import matplotlib.pyplot as plt\n",
    "    print(testset.classes)\n",
    "    original, adv_x, y, output = fgsm_attack(model, testloader, device, sample_id=0, eps=1/255, targeted_attack=True, target_label=testset.classes.index('deer'))\n",
    "\n",
    "    img = inv(adv_x.squeeze())\n",
    "    plt.imshow(img.permute(1, 2, 0).detach().cpu())\n",
    "    plt.title(testset.classes[output.argmax()])\n",
    "    plt.show()\n",
    "\n",
    "    diff = adv_x - original\n",
    "    diff_img = inv(diff[0])\n",
    "    plt.imshow(diff_img.permute(1, 2, 0).detach().cpu())\n",
    "    plt.title('Difference')\n",
    "    plt.show()\n",
    "\n",
    "    diff_flat = diff.flatten()\n",
    "    plt.hist(diff_flat.detach().cpu())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187c8f79-06a8-4ca4-a3f5-26e0367c802a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.579145800Z"
    }
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4500d6f-57f5-4d25-90f3-b29e2fbd6885",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.579145800Z"
    }
   },
   "outputs": [],
   "source": [
    "diff.squeeze().mean(0).shape\n",
    "plt.imshow(255*diff.cpu().detach().squeeze().mean(0))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b3bf62",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Augment training with adversarial examples\n",
    "\n",
    "Use your implementation of FGSM to augment your training dataset with adversarial samples. Ideally, you should implement this data augmentation *on the fly* so that the adversarial samples are always generated using the current model. Evaluate whether the model is more (or less) robust to ID samples using your OOD detection pipeline and metrics you implemented in Exercise 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cf2602e29f45f0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Goal\n",
    "To create a batch of adversarial examples by performing a **single-step gradient ascent** in the input space. The goal is to find the direction that will **maximize the model's loss** as quickly as possible and then take a small step in that direction, hopefully  fooling the classifier.\n",
    "\n",
    "#### Method\n",
    "The function executes the FGSM algorithm, which can be broken down into these key steps:\n",
    "\n",
    "1.  **Prepare for Gradient Calculation**:\n",
    "    *   A detached clone of the input tensor x is created, and its requires_grad attribute is set to True. This operation is critical as it designates the input image's pixels, rather than the model's weights, as the parameters with respect to which the gradients will be computed.\n",
    "\n",
    "2.  **Calculate the Loss**:\n",
    "    *   A standard forward pass is performed (`logits = model(x_adv)`), and the loss is calculated against the *true* labels `y`. \n",
    "\n",
    "3.  **Find the Direction of Attack**:\n",
    "    *   `torch.autograd.grad(loss, x_adv, ...)`: This is the core of the attack. It computes the gradient of the `loss` with respect to the input image `x_adv`. This gradient, `∇_x L(θ, x, y)`, is a tensor that has the same shape as the image and points in the direction in the pixel space that will cause the **greatest increase** in the loss.\n",
    "    *   `torch.sign(...)`: This is the \"Sign\" part of FGSM. Instead of using the full gradient, the algorithm takes only its sign (`-1` if negative, `+1` if positive). This identifies the general direction of the steepest ascent for each pixel, simplifying the attack and creating a uniform perturbation.\n",
    "\n",
    "4.  **Apply the Perturbation**:\n",
    "    *   `x_adv = x_adv + eps * grad_sign`: The original image is modified by adding the signed gradient, scaled by a small constant `eps` (epsilon). Epsilon controls the magnitude of the attack; a larger `eps` creates a more noticeable change in the image but makes the attack more potent. \n",
    "\n",
    "5.  **Ensure Image Validity**:\n",
    "    *   `torch.clamp(x_adv, -1, 1)`: The perturbation might push some pixel values outside their valid range (e.g., below -1 or above 1). This line clips the values to ensure that the resulting `x_adv` is still a valid image tensor.\n",
    "\n",
    "#### Result\n",
    "*   The function returns `x_adv`, a tensor representing a batch of **adversarial images**.\n",
    "*   These images are visually almost indistinguishable from the original images in `x`, as the changes are subtle (controlled by `eps`).\n",
    "*   However, when this `x_adv` tensor is fed back into the `model`, the model is highly likely to misclassify it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c224896f4f3a59",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.579145800Z"
    }
   },
   "outputs": [],
   "source": [
    "def fgsm_attack_batch(model, x, y, eps=1/255, loss_fn=None):\n",
    "    if loss_fn is None:\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    x_adv = x.detach().clone()\n",
    "    x_adv.requires_grad_(True)\n",
    "    logits = model(x_adv)\n",
    "    loss = loss_fn(logits, y)\n",
    "    grad_sign = torch.sign(torch.autograd.grad(loss, x_adv, retain_graph=False, create_graph=False)[0])\n",
    "    x_adv = x_adv + eps * grad_sign\n",
    "    x_adv = torch.clamp(x_adv, -1, 1)\n",
    "    return x_adv.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba643c830f9731ab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Goal  \n",
    "The purpose of this code is to evaluate how well a model performs under normal conditions and when exposed to adversarial attacks. It measures three main things:  \n",
    "- The model’s *accuracy on clean data*.  \n",
    "- The model’s *accuracy on adversarially perturbed data*.  \n",
    "- The *average loss* the model produces during evaluation.  \n",
    "\n",
    "### Method  \n",
    "1. **Preparation**  \n",
    "   - Tracking variables are initialized for counting correctly classified samples, total samples, adversarial results, and losses.  \n",
    "   - If no custom loss function is provided, the code uses `CrossEntropyLoss` with reduction set to `\"sum\"`.  \n",
    "\n",
    "2. **Main Loop (per batch of data)**  \n",
    "   - Input images `x` and labels `y` are moved onto the appropriate device (GPU).  \n",
    "   - Predictions are generated on **clean inputs** without gradient calculations (`torch.no_grad()`).  \n",
    "   - The number of correct predictions and the total loss are updated.  \n",
    "\n",
    "3. **Adversarial Evaluation**  \n",
    "   - A mask (`correct_mask`) selects only the samples the model classified correctly in the clean setting.  \n",
    "   - If adversarial attacks are enabled (`eps > 0`) and there are correctly classified samples, these images are perturbed using the **Fast Gradient Sign Method (FGSM)** through the function `fgsm_attack_batch`.  \n",
    "   - The perturbed images `x_adv` are passed through the model, and predictions are compared with the true labels again to measure adversarial robustness.  \n",
    "\n",
    "4. **Final Aggregation**  \n",
    "   - Clean accuracy is computed as the ratio of correct clean predictions to total samples.  \n",
    "   - Adversarial accuracy is computed as the ratio of correct adversarial predictions to total samples.  \n",
    "   - Average loss is computed by dividing the accumulated loss by the total number of samples.  \n",
    "\n",
    "\n",
    "### Results  \n",
    "At the end, the function returns three key metrics:  \n",
    "- **Clean Accuracy (`clean_acc`)**: the percentage of samples correctly classified without modification.  \n",
    "- **Adversarial Accuracy (`adv_acc`)**: the percentage of samples correctly classified after adversarial perturbation. This gives insight into model robustness.  \n",
    "- **Average Loss (`avg_loss`)**: the mean loss across all clean samples, representing overall prediction quality.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aaeee4cfecc336",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.580144800Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, device, eps, loss_fn=None):\n",
    "    model.eval()\n",
    "    \n",
    "    total_clean_correct = 0\n",
    "    total_adv_correct = 0\n",
    "    total_samples = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    if loss_fn is None:\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "    \n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "     \n",
    "        with torch.no_grad():\n",
    "            logits_clean = model(x)\n",
    "            preds_clean = torch.argmax(logits_clean, dim=1)\n",
    "        total_clean_correct += (preds_clean == y).sum().item()\n",
    "        total_loss += loss_fn(logits_clean, y).item()\n",
    "        total_samples += y.size(0)\n",
    "\n",
    "        \n",
    "        correct_mask = preds_clean.eq(y)  \n",
    "        \n",
    "        if correct_mask.any() and eps > 0:\n",
    "            x_to_attack = x[correct_mask]\n",
    "            y_to_attack = y[correct_mask]\n",
    "\n",
    "            x_adv = fgsm_attack_batch(model, x_to_attack, y_to_attack, eps=eps)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits_adv = model(x_adv)\n",
    "                preds_adv = torch.argmax(logits_adv, dim=1)\n",
    "            total_adv_correct += (preds_adv == y_to_attack).sum().item()\n",
    "    \n",
    "    clean_acc = total_clean_correct / max(total_samples, 1)\n",
    "    adv_acc = total_adv_correct / max(total_samples, 1)\n",
    "    avg_loss = total_loss / max(total_samples, 1)\n",
    "    \n",
    "    return clean_acc, adv_acc, avg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef794f78-b40b-4ccc-8d6d-2ad4abd4cb2e",
   "metadata": {},
   "source": [
    "### Goal  \n",
    "This function aims to train a model with adversarial training using the Fast Gradient Sign Method (FGSM). It improves the robustness of the model by using a combined loss on both clean inputs and their adversarially perturbed counterparts. The training also monitors performance on a validation set, including clean accuracy, adversarial accuracy, and out-of-distribution (OOD) detection metrics, while saving the best model based on validation loss.  \n",
    "\n",
    "### Method   \n",
    "- AdamW optimizer is initialized with weight decay, and a cosine annealing learning rate scheduler is set to decay the learning rate smoothly across the total epochs. Cross-entropy loss serves as the criterion.  \n",
    "- For each training epoch:  \n",
    "  - The model trains on batches from the training loader. For each batch, adversarial examples are generated using FGSM (`fgsm_attack_batch`) by perturbing inputs slightly in the direction of the gradient to maximize loss.  \n",
    "  - The loss is calculated as a weighted sum of clean input loss (`loss_clean`) and adversarial input loss (`loss_adv`), controlled by `alpha`.\n",
    "  - Backpropagation and optimizer step update model weights. Running metrics for loss and accuracy on clean inputs are tracked.  \n",
    "- After each epoch, the scheduler updates learning rate according to a cosine annealing schedule.  \n",
    "- Validation is performed using the provided `evaluate_model` function to get clean accuracy, adversarial accuracy, and average loss metrics. Additionally, an OOD evaluation function returns metrics such as AUROC and FPR@95TPR.  \n",
    "- Metrics are logged to TensorBoard via `SummaryWriter` for visualization.  \n",
    "- The model with the lowest validation loss is saved for future use.  \n",
    "- After all epochs, the best saved model is loaded and the training history dictionary with various tracked metric lists is returned.  \n",
    " \n",
    "### Results  \n",
    "- The training history contains epoch-wise loss and accuracy on both training and validation sets (clean and adversarial), plus validation loss.  \n",
    "- Per epoch console output provides detailed information including OOD evaluation metrics (AUROC and FPR@95TPR), clean accuracy, and validation loss to help analyze robustness improvements.  \n",
    "- The learning rate follows a smooth cosine decay over epochs, improving convergence stability.  \n",
    "- The final model loaded is the best version based on validation loss, optimized for both clean and adversarial robustness.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b5f73-3c3f-41ce-abae-1023d96e6fe2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.580144800Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "def train_with_fgsm(model, trainloader, valloader, device,\n",
    "                    epochs=10, eps=1/255, alpha=0.5, lr=0.1, wd =5e-4):\n",
    "\n",
    "    model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_acc\": [], \"adv_val_acc\": [], \"val_loss\": []}\n",
    "\n",
    "    best_val =math.inf\n",
    "    path = './cifar10_best_advtrained.pth'\n",
    "    best_epoch = 0\n",
    "    best_auroc = 0 \n",
    "    logdir = os.path.join(\"fgsm\", f\"FGSM_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "    writer = SummaryWriter(log_dir=logdir)\n",
    "\n",
    "    epoch_pbar = tqdm(range(1, epochs + 1), desc=\"Training\", unit=\"epoch\")\n",
    "\n",
    "    for epoch in epoch_pbar:\n",
    "        model.train()\n",
    "        running_loss, running_acc, n_samples = 0.0, 0.0, 0\n",
    "\n",
    "        for x, y in trainloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            x_adv = fgsm_attack_batch(model, x, y, eps=eps)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logits_clean = model(x)\n",
    "            loss_clean = criterion(logits_clean, y)\n",
    "            \n",
    "            logits_adv = model(x_adv)\n",
    "            loss_adv = criterion(logits_adv, y)\n",
    "            \n",
    "            loss = alpha *  loss_clean  + (1.0 - alpha) * loss_adv\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = torch.argmax(logits_clean, dim=1)\n",
    "            acc = (preds == y).float().mean().item()\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "            running_acc += acc * x.size(0)\n",
    "            n_samples += x.size(0)\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss = running_loss / n_samples\n",
    "        train_acc = running_acc / n_samples\n",
    "        val_acc_clean, val_acc_adv, avg_loss = evaluate_model(model, valloader, device, eps, criterion)\n",
    "        metric =  eval_OOD(model,valloader, fakeloader, device)\n",
    "        print(f\"{epoch}| AUROC: {metric['AUROC']:.4f} |  FPR@95TPR: {metric['FPR@95TPR']:.4f} | Accuarcy: {val_acc_clean:.4f} | Val Loss: {avg_loss:.4f}\")\n",
    "        auroc = metric['AUROC']\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc_clean)\n",
    "        history[\"adv_val_acc\"].append(val_acc_adv)\n",
    "        history[\"val_loss\"].append(avg_loss)\n",
    "        \n",
    "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/train\", train_acc, epoch)\n",
    "        writer.add_scalar(\"Accuracy/val_clean\", val_acc_clean, epoch)\n",
    "        writer.add_scalar(\"Accuracy/val_adv\", val_acc_adv, epoch)\n",
    "        writer.add_scalar(\"Loss/val\", avg_loss, epoch)\n",
    "        writer.add_scalar(\"AUROC FakeDataset-OOD\",auroc, epoch)\n",
    "        if avg_loss < best_val:\n",
    "            best_val = avg_loss\n",
    "            torch.save(model.state_dict(), path)\n",
    "            best_epoch = epoch\n",
    "            best_auroc = auroc \n",
    "        epoch_pbar.set_postfix([\n",
    "           (\"train_loss\", f\"{train_loss:.4f}\"),\n",
    "           (\"val_acc\", f\"{val_acc_clean:.4f}\"),\n",
    "           (\"adv_val_acc\", f\"{val_acc_adv:.4f}\"),\n",
    "            (\"val_loss\", f\"{avg_loss:.4f}\"),\n",
    "           (\"best_val\", f\"{best_val:.4f}\"),\n",
    "           (\"best_epoch\", f\"{best_epoch}\"),\n",
    "           (\"best_auroc\", f\"{best_auroc:.4f}\")\n",
    "        ])\n",
    "\n",
    "    writer.close()\n",
    "    print(f\"\\nAddestramento completato.\")\n",
    "    print(f\"Caricamento del modello migliore con accuratezza avversaria di validazione: {best_val:.4f}\")\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70b932ad40a3187",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.581146800Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    device =\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    #location = './cifar10_best_my.pth'\n",
    "    model = CNN().to(device)\n",
    "    #model.load_state_dict(torch.load(location))\n",
    "    acc_clean,accuracy_adv ,_  = evaluate_model(model, testloader, device,eps=8/255)\n",
    "    print(f'Accuracy on adversarial samples before FGSM training: {accuracy_adv:.4f}')\n",
    "    metric= eval_OOD(model,testloader, fakeloader, device)\n",
    "    print(f\"AUROC: {metric['AUROC']:.4f} |  FPR@95TPR: {metric['FPR@95TPR']:.4f} | Accuarcy: {acc_clean:.4f}\")\n",
    "   # plot_robustness(metric, title_prefix=\"FakeDataset-OOD device,\")\n",
    "\n",
    "    history = train_with_fgsm(model, trainloader, valloader, device,\n",
    "                               epochs=30, eps=32/255,\n",
    "                              alpha=0.5 , lr=0.003, \n",
    "                              wd =5e-4)\n",
    "\n",
    "    acc_clean, accuracy_adv, _ = evaluate_model(model, testloader, device,eps=8/255)\n",
    "    print(f'Accuracy on adversarial samples after FGSM training: {accuracy_adv:.4f}')\n",
    "    metric = eval_OOD(model, testloader, fakeloader, device)\n",
    "    print(f\"AUROC: {metric['AUROC']:.4f} |  FPR@95TPR: {metric['FPR@95TPR']:.4f} | Accuarcy: {acc_clean:.4f}\")\n",
    "    plot_curve(metric, title_prefix=\"FakeDataset-OOD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc083bd5b297697",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.582146500Z"
    }
   },
   "outputs": [],
   "source": [
    "    a , accuracy_adv, b = evaluate_model(model, testloader, device,eps=8/255)\n",
    "    print(f'Accuracy on adversarial samples after FGSM training: {accuracy_adv:.4f}')\n",
    "    print(a, b)\n",
    "    metrics_fake = eval_robustness(model, testloader, fakeloader)\n",
    "    plot_robustness(metrics_fake, title_prefix=\"FakeDataset-OOD\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc36b2e-8c28-4030-9f2b-718b8f8f6779",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.584143500Z"
    }
   },
   "outputs": [],
   "source": [
    "scores_id = compute_scores(model, testloader, max_logit).cpu().numpy()\n",
    "scores_ood  = compute_scores(model, fakeloader, max_logit).cpu().numpy()\n",
    "plt.hist(scores_id, density=True, alpha=0.5, bins=25)\n",
    "plt.hist(scores_ood, density=True, alpha=0.5, bins=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63091fd3",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 3: Wildcard\n",
    "\n",
    "You know the drill. Pick *ONE* of the following exercises to complete.\n",
    "\n",
    "### Exercise 3.1: Implement ODIN for OOD detection\n",
    "ODIN is a very simple approach, and you can already start experimenting by implementing a temperature hyperparameter in your base model and doing a grid search on $T$ and $\\varepsilon$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c9ccff365d41f5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.585144Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "device =\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CNN().to(device)\n",
    "model.load_state_dict(torch.load(f'./cifar10_best_P.pth')) # modello dato dal professore  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa66c8e-b22c-469c-914c-e9669d3e4936",
   "metadata": {},
   "source": [
    "\n",
    "#### Goal \n",
    "To compute the ODIN OOD detection score by actively amplifying the confidence scores of in-distribution (ID) samples more than out-of-distribution (OOD) samples. The hypothesis is that a small, gradient-based perturbation that aims to increase the model's confidence will have a much larger effect on an ID sample than on an OOD sample, thus creating a wider gap between their final scores. \n",
    "#### Method \n",
    "The function calculates the ODIN score for each image in a batch by following a multi-step process: \n",
    "1. **Gradient Calculation Setup**: Just like in an adversarial attack, the function enables gradient computation with respect to the input image `x` by setting `x.requires_grad = True`.\n",
    "2. **Part 1: Temperature Scaling (`T`)**:\n",
    "    * The model's output logits are divided by a temperature value `T` (typically `T > 1`).\n",
    "    *  **Effect**: A high temperature \"softens\" the softmax distribution, making the probabilities less peaky. This might seem counterintuitive, but it has the effect of scaling up the magnitudes of the gradients for all classes, which is crucial for the next step.\n",
    "    * The loss is then calculated between these scaled logits and the model's *own prediction* (`y_pred_correct`).\n",
    "4. **Part 2: Input Pre-processing (`eps`)**:\n",
    "    * **Gradient Computation**: The function computes the gradient of the loss with respect to the input image `x`. This gradient points in the direction that would *increase* the loss.\n",
    "    * **Perturbation**: The key step of ODIN is `perturbed_x = x_correct - eps * gradient_sign`. This looks like FGSM but with a critical difference: the gradient is **subtracted**, not added\n",
    "    * **Reasoning**: We are performing **gradient descent** on the loss with respect to the input. This means we are subtly modifying the image to push it *even closer* to the decision boundary of its predicted class, with the goal of **maximizing the softmax score**. The core hypothesis of ODIN is that this small push has a much larger effect on ID samples  than on OOD samples.\n",
    "6. **Final Score Calculation**:\n",
    "   * A final forward pass is performed on the newly created `perturbed_x`. * The logits from this pass are again scaled by the temperature `T`\n",
    "   * The final ODIN score is the maximum value of the softmax output of this **perturbed, temperature-scaled** result.\n",
    "#### Result \n",
    "* The function returns a tensor of ODIN scores, one for each image.\n",
    "* These scores are not just a passive measure of the model's confidence; they are the result of an **active process** designed to separate ID and OOD samples.\n",
    "* The expected outcome is that the distribution of these ODIN scores for ID data will be much higher and more clearly separated from the distribution of scores for OOD data when compared to the simple MSP baseline. This should translate directly to superior quantitative metrics (higher AUROC, lower FPR@95TPR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09358e2-0310-40ec-838b-228838647061",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.586143600Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_scores_ODIN(model, dataloader, device, T, eps, filter_correct_only=False):\n",
    "    model.eval()\n",
    "    all_scores = []\n",
    "    for x, y_true in dataloader:\n",
    "        x, y_true = x.to(device), y_true.to(device)\n",
    "        x.requires_grad = True\n",
    "        logits = model(x)\n",
    "        y_pred = logits.argmax(dim=1)\n",
    "        if filter_correct_only:\n",
    "            correct_mask = y_pred.eq(y_true)\n",
    "        else: \n",
    "            correct_mask =  torch.ones(len(x), dtype=torch.bool)\n",
    "            \n",
    "        if correct_mask.sum() == 0:\n",
    "            continue\n",
    "            \n",
    "        logits_correct = logits[correct_mask]\n",
    "        y_pred_correct = y_pred[correct_mask]\n",
    "\n",
    "        loss = F.cross_entropy(logits_correct / T, y_pred_correct, reduction='sum')\n",
    "        grad_full, = torch.autograd.grad(loss, x)\n",
    "        x_correct = x[correct_mask]\n",
    "        grad_correct = grad_full[correct_mask]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gradient_sign = grad_correct.data.sign()\n",
    "            perturbed_x = x_correct - eps * gradient_sign/0.5\n",
    "           \n",
    "\n",
    "            final_logits = model(perturbed_x)\n",
    "            prob = F.softmax(final_logits / T, dim=1)\n",
    "            score = prob.max(dim=1)[0]\n",
    "            \n",
    "            all_scores.append(score.cpu())\n",
    "            \n",
    "    return torch.cat(all_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c6e44-a8a1-4ec0-8b9a-e1a84f03ea69",
   "metadata": {},
   "source": [
    "#### Goal\n",
    "To create a single, standardized figure that provides a comprehensive visual comparison of two OOD detection methods. The function should display both the **ROC curves** and the **Precision-Recall curves** for each method on shared axes, with clear legends that include the quantitative summary metrics (AUROC and AUPR).\n",
    "\n",
    "#### Method\n",
    "\n",
    "The function uses `matplotlib` and the convenient display objects from `sklearn.metrics` to build the visualization step-by-step:\n",
    "\n",
    "1.  **Figure Setup**: It begins by creating a figure with one row and two columns of subplots (`plt.subplots(1, 2, ...)`). This sets up the side-by-side layout for the ROC and Precision-Recall comparisons.\n",
    "\n",
    "2.  **Data Extraction**: It unpacks the raw data points (`fpr`, `tpr`, `recall`, `precision`) from the two input metric dictionaries. These raw points are what will be used to draw the curves.\n",
    "\n",
    "3.  **ROC Curve Plotting (Left Subplot)**:\n",
    "    *   It uses `sklearn.metrics.RocCurveDisplay`. This is a helper object that takes the `fpr`, `tpr`, and the pre-computed `roc_auc` score.\n",
    "    *   It plots the curve for the first method on the left subplot (`ax=ax1`) with a solid line.\n",
    "    *   It then plots the curve for the second method on the **same subplot**, but with a dashed line (`linestyle='--'`) to visually distinguish it.\n",
    "    *   **Informative Legend**: Instead of a simple legend, it creates a custom one that includes not just the label of each method but also its corresponding AUROC score, formatted to four decimal places. This adds crucial quantitative information directly to the plot.\n",
    "\n",
    "4.  **Precision-Recall Curve Plotting (Right Subplot)**:\n",
    "    *   It follows the exact same logic as the ROC plotting, but this time using `sklearn.metrics.PrecisionRecallDisplay` and targeting the right subplot (`ax=ax2`).\n",
    "    *   It overlays the two curves using solid and dashed lines and creates a custom legend that includes the AUPR score for each method.\n",
    "\n",
    "5.  **Final Touches**: `plt.tight_layout()` is called to automatically adjust the spacing and prevent titles and labels from overlapping, and `plt.show()` displays the final, completed figure.\n",
    "\n",
    "#### Result\n",
    "The function does not return any data; its result is the **visualization itself**. The output is a single, clear figure containing two plots:\n",
    "\n",
    "*   **Left Plot (ROC Comparison)**: This plot allows for an immediate visual assessment of which method is better at separating the two classes. The curve that is \"higher and to the left\" represents the superior detector. The legend provides the precise AUROC value to confirm the visual impression.\n",
    "*   **Right Plot (Precision-Recall Comparison)**: This plot provides a complementary view of performance, which is especially useful when datasets are imbalanced. The curve that is \"higher and to the right\" is better. The legend provides the AUPR value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f386c418c3c428",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.587143800Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_curve_comparison(metrics_dict1, metrics_dict2, label1=\"Method 1\", label2=\"Method 2\", title_prefix=\"\"):\n",
    "\n",
    "    auc_digits = 4\n",
    "    \n",
    "    fpr1, tpr1 = metrics_dict1[\"roc_curve\"]\n",
    "    recall1, precision1 = metrics_dict1[\"pr_curve\"]\n",
    "    \n",
    "    fpr2, tpr2 = metrics_dict2[\"roc_curve\"]\n",
    "    recall2, precision2 = metrics_dict2[\"pr_curve\"]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    " \n",
    "    disp1 = RocCurveDisplay(fpr=fpr1, tpr=tpr1, roc_auc=metrics_dict1[\"AUROC\"])\n",
    "    disp1.plot(ax=ax1, name=f'{label1}', curve_kwargs={'linestyle': '-'})\n",
    "    \n",
    "    disp2 = RocCurveDisplay(fpr=fpr2, tpr=tpr2, roc_auc=metrics_dict2[\"AUROC\"])\n",
    "    disp2.plot(ax=ax1, name=f'{label2}', curve_kwargs={'linestyle': '--'})\n",
    "    \n",
    "    ax1.set_title(f\"{title_prefix} ROC Curve Comparison\")\n",
    "    ax1.legend([\n",
    "        f\"{label1} (AUROC = {metrics_dict1['AUROC']:.{auc_digits}f})\",\n",
    "        f\"{label2} (AUROC = {metrics_dict2['AUROC']:.{auc_digits}f})\"\n",
    "    ], loc=\"lower right\")\n",
    "    \n",
    "    disp_pr1 = PrecisionRecallDisplay(precision=precision1, recall=recall1)\n",
    "    disp_pr1.plot(ax=ax2, name=f'{label1}', linestyle='-')\n",
    "    \n",
    "    disp_pr2 = PrecisionRecallDisplay(precision=precision2, recall=recall2)\n",
    "    disp_pr2.plot(ax=ax2, name=f'{label2}', linestyle='--')\n",
    "    \n",
    "    ax2.set_title(f\"{title_prefix} Precision-Recall Curve Comparison\")\n",
    "    ax2.legend([\n",
    "        f\"{label1} (AUPR = {metrics_dict1['AUPR']:.{auc_digits}f})\",\n",
    "        f\"{label2} (AUPR = {metrics_dict2['AUPR']:.{auc_digits}f})\"\n",
    "    ], loc=\"lower left\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58891b24-3864-4b6d-be34-7a2c55bd935c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.588143Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'./cifar10_best_P.pth'))\n",
    "metric1 = eval_OOD(model, testloader, fakeloader, device, compute_scores_ODIN)\n",
    "print(f\"  FPR at 95 TPR: {metric1['FPR@95TPR']:.4f} | Detection Error at 95 tpr: {metric1['DetectionError@95TPR']:.4f}\")\n",
    "\n",
    "metric2 = eval_OOD(model, testloader, fakeloader, device,compute_scores)\n",
    "print(f\"  FPR at 95 TPR: {metric2['FPR@95TPR']:.4f} | Detection Error at 95 tpr: {metric2['DetectionError@95TPR']:.4f}\")\n",
    "plot_curve_comparison(metric1,metric2,\"ODIN\", \"Baseline\", title_prefix=\"CNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef5bfce-63a2-404b-bfe8-1ea397e3f6ef",
   "metadata": {},
   "source": [
    "#### Goal\n",
    "To systematically test a wide range of `temperature` and `eps` values to find the single best pair that minimizes the False Positive Rate when the True Positive Rate is fixed at 95% (FPR@95TPR). This process tunes the ODIN algorithm to its peak performance for the specific model and dataset being used.\n",
    "\n",
    "#### Method\n",
    "The script implements a classic **grid search** algorithm:\n",
    "\n",
    "1.  **Define the Search Space**: Two lists of potential hyperparameter values are created: `temperature` and `all_eps`. These lists define the \"grid\" of all possible combinations that will be tested.\n",
    "\n",
    "2.  **Iterate Through the Grid**: The code uses nested `for` loops to iterate through every single `(T, eps)` pair in the search space.\n",
    "\n",
    "3.  **Evaluate Each Combination**: For each pair of hyperparameters, the full OOD evaluation is performed:\n",
    "    *   The `compute_scores_ODIN` function is called for both the in-distribution (`testloader`) and out-of-distribution (`fakeloader`) data using the current `T` and `eps`.\n",
    "    *   The ground truth labels and predicted scores are assembled.\n",
    "    *   The ROC curve is calculated, and the specific metric of interest, `fpr_at_95_ODIN`, is extracted. This metric represents the performance of the current `(T, eps)` configuration.\n",
    "\n",
    "4.  **Track the Best Performance**: A variable, `best_fpr`, is initialized to a very high value. After each evaluation, the script checks:\n",
    "    *   `if fpr_at_95_ODIN < best_fpr:`\n",
    "    *   If the current combination of hyperparameters yields a lower (better) FPR than the best one found so far, the script updates `best_fpr` and stores the current `T` and `eps` as the new best values (`bestT`, `bestEps`).\n",
    "    *   The print statements provide a real-time log of the search, indicating when a new best combination has been found.\n",
    "\n",
    "#### Result\n",
    "The primary result of this script is not a trained model, but a set of **optimal hyperparameters**.\n",
    "\n",
    "*   **`bestT` and `bestEps`**: The final output provides the single `Temperature` and `Epsilon` values that produced the best OOD detection performance out of all the combinations tested.\n",
    "*   **`best_fpr`**: The script also outputs the best achievable performance score (the minimum FPR@95TPR). This value represents the peak performance of the ODIN method after it has been properly tuned.\n",
    "*   **Scores for Plotting (`best_ypred`, `best_ytrue`)**: The script conveniently saves the raw scores from the single best run. These can be passed directly to the `eval_OOD` function to generate a full metrics dictionary and then to the `plot_curve_comparison` function to visualize the performance of the **optimized** ODIN method against other techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcad0f2-071c-457a-b44d-1d0eee6414ea",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.589144300Z"
    }
   },
   "outputs": [],
   "source": [
    "temperature = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "all_eps =  np.linspace(0, 0.004, 21)\n",
    "\n",
    "std_dev = 0.5\n",
    "best_fpr = 100\n",
    "n=0\n",
    "tot = len (temperature) * len(all_eps) \n",
    "for T in temperature:\n",
    "    for eps in all_eps:\n",
    "\n",
    "        \n",
    "        scores_test_ODIN = []\n",
    "        model.eval()\n",
    "        \n",
    "        scores_test_ODIN = compute_scores_ODIN(model, testloader,device, T, eps, filter_correct_only=True)\n",
    "        scores_fake_ODIN = compute_scores_ODIN(model, fakeloader,device, T, eps, filter_correct_only=False)    \n",
    "        \n",
    "\n",
    "        y_fake_ODIN = torch.zeros_like(scores_fake_ODIN)\n",
    "        y_test_ODIN = torch.ones_like(scores_test_ODIN)\n",
    "        ypred_ODIN = torch.cat((scores_test_ODIN, scores_fake_ODIN))\n",
    "        ytrue_ODIN = torch.cat((y_test_ODIN, y_fake_ODIN))\n",
    "        fpr_ODIN, tpr_ODIN, thresholds_ODIN = skm.roc_curve(ytrue_ODIN, ypred_ODIN.detach().numpy())\n",
    "        idx_ODIN = np.argmin(np.abs(tpr_ODIN - 0.95))\n",
    "        fpr_at_95_ODIN = fpr_ODIN[idx_ODIN]\n",
    "        n += 1\n",
    "       \n",
    "        if fpr_at_95_ODIN < best_fpr:\n",
    "            best_ypred = ypred_ODIN\n",
    "            best_ytrue = ytrue_ODIN\n",
    "            bestT = T\n",
    "            bestEps = eps\n",
    "            best_fpr = fpr_at_95_ODIN\n",
    "            print(f'Tempearture: {T}, Epsilon: {eps:.4f}, FPR at 95% TPR: {fpr_at_95_ODIN:.6f}  {n}/{tot}  new Best') \n",
    "        else :\n",
    "            print(f'Tempearture: {T}, Epsilon: {eps:.4f}, FPR at 95% TPR: {fpr_at_95_ODIN:.6f}  {n}/{tot}') \n",
    "            \n",
    "    print()\n",
    "    print()\n",
    "\n",
    "print( f'Best performance Temperature:{bestT}  Epsilon: {bestEps:.4f}, FPR at 95% TPR:{best_fpr}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf98826-0f9b-462e-8c03-d3aee5a0e005",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.590143600Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "fpr_ODIN, tpr_ODIN, _ = metrics.roc_curve(best_ytrue, best_ypred.detach().cpu().numpy())\n",
    "roc_auc_ODIN = metrics.auc(fpr_ODIN, tpr_ODIN)\n",
    "\n",
    "# Prepare true labels and predictions for regular scores\n",
    "y_fake = torch.zeros_like(scores_fake)\n",
    "y_test = torch.ones_like(scores_test)\n",
    "ypred = torch.cat((scores_test, scores_fake))\n",
    "ytrue = torch.cat((y_test, y_fake))\n",
    "print( ytrue, ypred) \n",
    "ytrue = ytrue.cpu()\n",
    "ypred = ypred.cpu()\n",
    "# Compute ROC curve and AUC for regular scores\n",
    "fpr, tpr, _ = metrics.roc_curve(ytrue, ypred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# Plot both ROC curves on the same figure\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_ODIN, tpr_ODIN, label=f'ODIN ROC (AUC = {roc_auc_ODIN:.4f})')\n",
    "plt.plot(fpr, tpr, label=f'Baseline ROC (AUC = {roc_auc:.4f})')\n",
    "\n",
    "# Plot diagonal line\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "# Plot formatting\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cddc84e-6295-49bd-befd-b279036f50e5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.591145900Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1f49e8-16d4-41f2-8a87-81ba26e62a57",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.592144400Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd85683-b9b4-47e2-be4a-67f873bb47c2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-18T21:09:42.593148200Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
