{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aba9d646-8c99-4ca6-b666-c7248c5d93ae",
   "metadata": {},
   "source": [
    "### Exercise 1: Sentiment Analysis (warm up)\n",
    "\n",
    "In this first exercise we will start from a pre-trained BERT transformer and build up a model able to perform text sentiment analysis. Transformers are complex beasts, so we will build up our pipeline in several explorative and incremental steps.\n",
    "\n",
    "#### Exercise 1.1: Dataset Splits and Pre-trained model\n",
    "There are a many sentiment analysis datasets, but we will use one of the smallest ones available: the [Cornell Rotten Tomatoes movie review dataset](cornell-movie-review-data/rotten_tomatoes), which consists of 5,331 positive and 5,331 negative processed sentences from the Rotten Tomatoes movie reviews.\n",
    "\n",
    "**Your first task**: Load the dataset and figure out what splits are available and how to get them. Spend some time exploring the dataset to see how it is organized. Note that we will be using the [HuggingFace Datasets](https://huggingface.co/docs/datasets/en/index) library for downloading, accessing, splitting, and batching data for training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8328f8ed-c2f3-4584-ae60-bc4dc95952f7",
   "metadata": {},
   "source": [
    "first of all set the seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fc021e2-f3a6-4d7c-aac4-42c82209f58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "seed = 2\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc515855-8fb6-4505-a7b4-b07a632f28c8",
   "metadata": {},
   "source": [
    "- **Goal**: To inspect the available data splits (e.g., train, test) for the Rotten Tomatoes dataset.\n",
    "- **First Step**: We use the lightweight function get_dataset_split_names to find out what data splits are available.\n",
    "- **Key Feature**: What makes this function so useful is that it does not download the entire dataset. Instead, it only fetches the metadata, which is much faster and more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0af8ab0f-d63d-4e72-ab83-581a9bbe3283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'validation', 'test']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, get_dataset_split_names\n",
    "\n",
    "split =get_dataset_split_names(\"cornell-movie-review-data/rotten_tomatoes\")\n",
    "split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf27c51-b1ae-43c1-80ff-715e115904cc",
   "metadata": {},
   "source": [
    "*   **Goal**: To download and load the **`train` split** of the Rotten Tomatoes dataset into a variable.\n",
    "*   **Method**: We use the `load_dataset` function, which now downloads the actual data.\n",
    "    *   The first argument, `\"cornell-movie-review-data/rotten_tomatoes\"`, tells the function *which* dataset to get.\n",
    "    *   The `split=\"train\"` argument tells it to download *only* the training portion.\n",
    "    *   The resulting dataset object is stored in the variable `ds`.\n",
    "*   **Result**: Displaying the `ds` variable in a notebook provides a rich summary of the loaded data, including:\n",
    "    *   **`Features`**: The columns of the dataset (in this case, `text` and `label`).\n",
    "    *   **`num_rows`**: The total number of examples in the training split (e.g., `8530`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8e48b15-e062-4fd4-a665-477ad96554a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 8530\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\", split =\"train\")\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95a7115-945d-4c5b-96c3-34fc5c2777f5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "*   **Goal**: To prepare the data for efficient batch processing in a PyTorch model.\n",
    "*   **Method**: First, use `ds.set_format` to convert the data into PyTorch `Tensors`. Then, use `DataLoader` to group those tensors into batches of 256.\n",
    "*   **Result**: The `trainloader` variable becomes an efficient iterable that feeds batches of data, perfectly formatted for training a PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45fc56da-b1c1-45f3-a7f9-d50aac90bb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "ds.set_format(type=\"torch\" , columns = [\"text\", \"label\"])\n",
    "trainloader =  DataLoader( ds, batch_size = 256 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1214a2f8-e74d-4023-b61c-1f2c96eb400a",
   "metadata": {},
   "source": [
    "Now, let's test the `trainloader` by pulling the first batch and checking the first review's text and the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7861c43d-a66f-41c6-8f82-f356a556df75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . tensor(1)\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "for data in trainloader:\n",
    "    text,y = data[\"text\"], data[\"label\"]\n",
    "    print(text[0] , y[0])\n",
    "    print(len(text))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6025d1a-927e-449c-b8f5-c22d0822528d",
   "metadata": {},
   "source": [
    "#### Exercise 1.2: A Pre-trained BERT and Tokenizer\n",
    "\n",
    "The model we will use is a *very* small BERT transformer called [Distilbert](https://huggingface.co/distilbert/distilbert-base-uncased) this model was trained (using self-supervised learning) on the same corpus as BERT but using the full BERT base model as a *teacher*.\n",
    "\n",
    "**Your next task**: Load the Distilbert model and corresponding tokenizer. Use the tokenizer on a few samples from the dataset and pass the tokens through the model to see what outputs are provided. I suggest you use the [`AutoModel`](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html) class (and the `from_pretrained()` method) to load the model and `AutoTokenizer` to load the tokenizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c9a620-c017-4466-a5a3-d72efe6fe315",
   "metadata": {},
   "source": [
    "\n",
    "*   **Goal**: To load a pre-trained DistilBERT model and its matching tokenizer, and to prepare the model for fast computation on a GPU.\n",
    "\n",
    "*   **Method**:\n",
    "    *   We use `AutoModel.from_pretrained(...)` to download and instantiate the DistilBERT model.\n",
    "    *   We immediately call `.to(\"cuda\")` to move the model's parameters to the GPU. This is crucial for performance.\n",
    "    *   We use `AutoTokenizer.from_pretrained(...)` with the **exact same model name** to ensure the tokenizer's vocabulary perfectly matches the model it was trained with.\n",
    "\n",
    "*   **Result**:\n",
    "    *   The `model` variable now holds the complete DistilBERT architecture with its pre-trained weights, residing on the GPU and ready to process data.\n",
    "    *   The `tokenizer` variable holds an object that knows exactly how to convert sentences into the specific numerical format that our `model` expects.\n",
    "    *   Displaying these objects in a notebook provides a summary of their configuration, confirming they have loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88c774fb-059e-4ca7-86e6-8620c1473877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): DistilBertSdpaAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "model = AutoModel.from_pretrained(\"distilbert/distilbert-base-uncased\").to(\"cuda\")\n",
    "tokenizer =  AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1acc3662-3a9b-4957-b8ce-26cbb3f801f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a714a2-730a-40ec-8172-086bed708181",
   "metadata": {},
   "source": [
    "We encode a batch of text into padded PyTorch tensors (`input_ids`) for the GPU, then immediately use `batch_decode` to convert them back to strings. This round-trip process lets us verify the transformation and confirm the correct application of special tokens like `[CLS]`, `[SEP]`, and `[PAD]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "380358a2-0d4c-4c93-8d54-fedcb982d008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "tensor([  101,  1996,  2600,  2003, 16036,  2000,  2022,  1996,  7398,  2301,\n",
      "         1005,  1055,  2047,  1000, 16608,  1000,  1998,  2008,  2002,  1005,\n",
      "         1055,  2183,  2000,  2191,  1037, 17624,  2130,  3618,  2084,  7779,\n",
      "        29058,  8625, 13327,  1010,  3744,  1011, 18856, 19513,  3158,  5477,\n",
      "         4168,  2030,  7112, 16562,  2140,  1012,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0], device='cuda:0')\n",
      "[CLS] the rock is destined to be the 21st century ' s new \" conan \" and that he ' s going to make a splash even greater than arnold schwarzenegger, jean - claud van damme or steven segal. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "lenght: 177 62 283\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t_text = tokenizer(text,return_tensors=\"pt\", padding=True ).to(\"cuda\")\n",
    "text_d= tokenizer.batch_decode(t_text[\"input_ids\"])\n",
    "print(text[0])\n",
    "print(t_text[\"input_ids\"][0])\n",
    "print(text_d[0])\n",
    "print(\"lenght:\", len(text[0]), len(t_text[\"input_ids\"][0]), len(text_d[0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1bcbd9-5f97-4153-ad4b-2582e00545da",
   "metadata": {},
   "source": [
    "After tokenizing the text, we feed it to the DistilBERT model. The model processes the input and, for every token in every sequence, it outputs a dense vector of 768 numbers. This vector is the \"contextual embedding.\"\n",
    "\n",
    "The shape of the output tensor, `y_pred.last_hidden_state.shape`, confirms this: `[batch_size, sequence_length, 768]`. Each of the 768-dimensional vectors represents a token's meaning within a high-dimensional vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23665c2b-69c7-419c-bb6c-540a1184cd76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 62, 768])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model(**t_text)\n",
    "y_pred.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be625ce3-4bc1-4832-8558-3a87694ca31b",
   "metadata": {},
   "source": [
    "#### Exercise 1.3: A Stable Baseline\n",
    "\n",
    "In this exercise I want you to:\n",
    "1. Use Distilbert as a *feature extractor* to extract representations of the text strings from the dataset splits;\n",
    "2. Train a classifier (your choice, by an SVM from Scikit-learn is an easy choice).\n",
    "3. Evaluate performance on the validation and test splits.\n",
    "\n",
    "These results are our *stable baseline* -- the **starting** point on which we will (hopefully) improve in the next exercise.\n",
    "\n",
    "**Hint**: There are a number of ways to implement the feature extractor, but probably the best is to use a [feature extraction `pipeline`](https://huggingface.co/tasks/feature-extraction). You will need to interpret the output of the pipeline and extract only the `[CLS]` token from the *last* transformer layer. *How can you figure out which output that is?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df25f93-f854-48da-be60-a8b8f4588c84",
   "metadata": {},
   "source": [
    "*   **Goal**: To extract the same 768-dimensional feature vectors as before, but using a simpler, automated workflow.\n",
    "\n",
    "*   **Method**:\n",
    "    *   We initialize a `pipeline` and tell it we want to perform the `\"feature-extraction\"` task. This specific task tells the pipeline to return the model's hidden states (the embeddings).\n",
    "    *   We specify the `model` checkpoint and `framework` (\"pt\" for PyTorch), just like before.\n",
    "    *   Crucially, instead of tokenizing the text ourselves, we can now pass the raw list of `text` strings **directly** to the `feature_extractor` object. The pipeline handles all the necessary steps (tokenization, padding, moving to the GPU, and running the model) under the hood.\n",
    "\n",
    "*   **Result**:\n",
    "    *   The output `y_pred` is a **list of PyTorch tensors**, where each tensor corresponds to one of the sentences in the input `text`.\n",
    "    *   Each tensor has a shape of `[1, sequence_length, 768]`, containing the vector embeddings for every token in that specific sentence.\n",
    "    *   This is a key difference from the manual approach. In that method, we **had to use `padding=True`** to force all sequences into a single, large tensor of shape `[batch_size, padded_length, 768]` for the model to accept the batch.\n",
    "    *   The `pipeline` handles this differently. It returns a **list of un-padded tensors**, each with its original, variable sequence length. This is often more convenient as you don't have to deal with masking or removing padding tokens yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe05e0a3-93f1-4d57-8c10-1cbb186aeadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 47, 768])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "checkpoint = \"distilbert/distilbert-base-uncased\"\n",
    "feature_extractor = pipeline(\"feature-extraction\", framework=\"pt\", model=checkpoint)\n",
    "tokenizer = feature_extractor.tokenizer\n",
    "y_pred= feature_extractor(text,return_tensors = \"pt\")\n",
    "y_pred[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a753530c-7e53-419d-b7e3-7b53e0c6bd64",
   "metadata": {},
   "source": [
    "*   **Goal**: To systematically iterate through all training data, convert each text review into a single 768-dimensional feature vector, and collect these vectors and their corresponding labels into two final NumPy arrays, `X` and `y`.\n",
    "\n",
    "*   **Method**:\n",
    "    1.  **Looping with `tqdm`**: We wrap our `trainloader` in `tqdm` to create a helpful progress bar, showing us how much of the dataset has been processed.\n",
    "    2.  **Feature Extraction**: Inside the loop, we pass a `text_batch` to our `feature_extractor` pipeline, just as we did in the test.\n",
    "    3.  **Isolating the `[CLS]` Token**: For a classification task, a standard technique is to use the embedding of the special `[CLS]` token (which the tokenizer adds to the beginning of every sequence) as a summary representation of the entire sentence. The list comprehension `[f[0] for f in features]` efficiently extracts this first vector from each review's feature set.\n",
    "    4.  **Aggregation**: The features (now NumPy arrays) and labels from each batch are appended to the `X_feats` and `y_all` lists.\n",
    "    5.  **Concatenation**: After the loop finishes, we have lists of arrays (one array per batch). `np.concatenate` is used to stack these arrays together along `axis=0` to create one single, continuous NumPy array for `X` and one for `y`.\n",
    "\n",
    "*   **Result**:\n",
    "    *   `X`: A single NumPy array with a shape like `(8530, 768)`. Each of the 8530 rows is a 768-dimensional vector representing one movie review.\n",
    "    *   `y`: A single NumPy array with a shape like `(8530,)`, containing the corresponding label (0 for negative, 1 for positive) for each review.\n",
    "    *   This `X` and `y` pair is now a classic machine learning dataset, perfectly formatted to be used with models from libraries like **Scikit-learn** (e.g., Logistic Regression, SVM, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7199417-f969-4db9-abf6-e0f990dae4f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f39fddad50248c8b896f35f2d49ba4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8530, 768) (8530,)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "X_feats =[]\n",
    "y_all = []\n",
    "for batch in tqdm(trainloader):\n",
    "    \n",
    "    text_batch = batch[\"text\"]\n",
    "    label_batch = batch[\"label\"]\n",
    "    features = feature_extractor(text_batch,return_tensors = \"pt\")\n",
    "    features_CLS = [ f[:,0,:] for f in features] \n",
    "    a = np.array(features_CLS)\n",
    " \n",
    "    X_feats.append(a)\n",
    "    y_all.append(label_batch.cpu().numpy())\n",
    "\n",
    "\n",
    "\n",
    "X = np.concatenate(X_feats, axis=0)\n",
    "X = np.concatenate(X, axis = 0)\n",
    "y = np.concatenate(y_all, axis=0)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c603546-e03a-4901-af5f-0940e99b54c7",
   "metadata": {},
   "source": [
    "*   **Goal**: To train a machine learning model that can learn the relationship between the 768-dimensional text embeddings (`X`) and their corresponding sentiment labels (`y`).\n",
    "*   **Method**:\n",
    "    1.  **`svm.SVC(kernel=\"linear\")`**: We create an instance of a Support Vector Classifier. By setting `kernel=\"linear\"`, we are instructing the model to find a simple linear boundary (a hyperplane) to separate the two classes (positive and negative). \n",
    "    2.  **`clf.fit(X, y)`**: This is the core training step. The `.fit()` method takes our entire feature matrix `X` and the label vector `y` and runs the SVM algorithm. The algorithm works to find the optimal hyperplane that best separates the data points of the two classes in the 768-dimensional space.\n",
    "*   **Result**:\n",
    "    *   The `svm_CLS` object is no longer an empty blueprint; it is now a **fully trained classifier**.\n",
    "    *   It has learned the specific parameters that define the decision boundary between positive and negative reviews based on the features extracted by DistilBERT.\n",
    "    *   This trained `svm_CLS` is now ready to make predictions on new, unseen movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4771628f-2448-4b9e-a277-4bf0731cb61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SVC</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('C',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">C&nbsp;</td>\n",
       "            <td class=\"value\">1.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('kernel',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">kernel&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;linear&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('degree',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">degree&nbsp;</td>\n",
       "            <td class=\"value\">3</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('gamma',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">gamma&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;scale&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('coef0',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">coef0&nbsp;</td>\n",
       "            <td class=\"value\">0.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('shrinking',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">shrinking&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('probability',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">probability&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tol',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">tol&nbsp;</td>\n",
       "            <td class=\"value\">0.001</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('cache_size',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">cache_size&nbsp;</td>\n",
       "            <td class=\"value\">200</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('class_weight',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">class_weight&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_iter',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_iter&nbsp;</td>\n",
       "            <td class=\"value\">-1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('decision_function_shape',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">decision_function_shape&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;ovr&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('break_ties',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">break_ties&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">random_state&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svm_CLS = svm.SVC(kernel=\"linear\")  \n",
    "svm_CLS.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db04278-ad1f-4112-9a36-6b00610562a8",
   "metadata": {},
   "source": [
    "To evaluate our trained model, we must now apply the exact same feature extraction process to the validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7771ad40-3c1f-454f-b682-b1e53880a51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_val = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\", split =\"validation\")\n",
    "ds_val.set_format(type=\"torch\" , columns = [\"text\", \"label\"])\n",
    "valloader =  DataLoader(ds_val, batch_size = 256, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a063355-21ed-47fe-8cd3-8fe95a89920b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb02532b32f44ca0893ba94a56e62178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_feats_val =[]\n",
    "y_all_val= []\n",
    "for batch in tqdm(valloader):\n",
    "    text_batch = batch[\"text\"]\n",
    "    label_batch = batch[\"label\"]\n",
    "\n",
    "    features = feature_extractor(text_batch,return_tensors = \"pt\")\n",
    "\n",
    "    features_CLS = [ f[:,0,:] for f in features] \n",
    "    a = np.array(features_CLS)\n",
    "    X_feats_val.append(a)\n",
    "    y_all_val.append(label_batch.cpu().numpy())\n",
    "\n",
    "\n",
    "\n",
    "X_val = np.concatenate(X_feats_val, axis=0)\n",
    "X_val = np.concatenate(X_val, axis=0)\n",
    "y_val = np.concatenate(y_all_val, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdbda89-f74b-412f-89fb-2f9aab082fb0",
   "metadata": {},
   "source": [
    "*   **Goal**: To measure the accuracy of our trained SVM classifier on the unseen validation data.\n",
    "*   **Method**:\n",
    "    1.  **`clf.predict(X_val)`**: We take the feature vectors of our validation set (`X_val`) and pass them to the `.predict()` method of our trained classifier (`clf`). The model uses the decision boundary it learned during training to assign a predicted label (0 or 1) to each review.\n",
    "    2.  **`accuracy_score(y_val, y_pred)`**: This function from Scikit-learn takes the model's predictions (`y_pred`) and the true ground-truth labels (`y_val`) and calculates the percentage of times the prediction was correct.\n",
    "    3.  **`print(...)`**: The final step is to print this accuracy score, formatted to four decimal places for readability.\n",
    "*   **Result**:\n",
    "    *   The output is a single floating-point number.\n",
    "    *   This score represents the model's **accuracy**—the proportion of reviews in the validation set that our model classified correctly.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed935420-6f20-42ec-a32e-eb79e3b5a432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuratezza sul validation set: 0.8189\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = svm_CLS.predict(X_val)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(f\"Accuratezza sul validation set: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c01217b-d85a-48df-ae7b-ebbefda72a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\", split =\"test\")\n",
    "ds_test.set_format(type=\"torch\" , columns = [\"text\", \"label\"])\n",
    "testloader =  DataLoader(ds_test, batch_size = 256, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0f872a7-ee6e-45bf-b03b-354865eddc89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b239f1135b452aa8d3b37d969a672d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_feats_test =[]\n",
    "y_all_test= []\n",
    "for batch in tqdm(testloader):\n",
    "    text_batch = batch[\"text\"]\n",
    "    label_batch = batch[\"label\"]\n",
    "\n",
    "    features = feature_extractor(text_batch,return_tensors = \"pt\")\n",
    "\n",
    "    features_CLS = [ f[:,0,:] for f in features] \n",
    "    a = np.array(features_CLS)\n",
    "    #print(len(features_CLS), features_CLS[0].shape, a.shape)\n",
    "    X_feats_test.append(a)\n",
    "    y_all_test.append(label_batch.cpu().numpy())\n",
    "\n",
    "\n",
    "\n",
    "X_test = np.concatenate(X_feats_test, axis=0)\n",
    "X_test = np.concatenate(X_test, axis=0)\n",
    "y_test = np.concatenate(y_all_test, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9dc291e5-3c75-4844-b9a2-9e5b86a5113b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuratezza sul test set: 0.8068\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = svm_CLS.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuratezza sul test set: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77031181-9ae0-4e6a-b32f-d56d3e092bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2eec61e-79f1-491f-b8d4-093f61ed613e",
   "metadata": {},
   "source": [
    "-----\n",
    "### Exercise 2: Fine-tuning Distilbert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fd83f4-456c-4e38-aa01-2687a19fa06c",
   "metadata": {},
   "source": [
    "In this exercise we will fine-tune the Distilbert model to (hopefully) improve sentiment analysis performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c65f4c-542d-40b0-9f04-f8c50019f2e0",
   "metadata": {},
   "source": [
    "#### Exercise 2.1: Token Preprocessing\n",
    "\n",
    "The first thing we need to do is *tokenize* our dataset splits. Our current datasets return a dictionary with *strings*, but we want *input token ids* (i.e. the output of the tokenizer). This is easy enough to do my hand, but the HugginFace `Dataset` class provides convenient, efficient, and *lazy* methods. See the documentation for [`Dataset.map`](https://huggingface.co/docs/datasets/v3.5.0/en/package_reference/main_classes#datasets.Dataset.map).\n",
    "\n",
    "**Tip**: Verify that your new datasets are returning for every element: `text`, `label`, `intput_ids`, and `attention_mask`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e38a63c-79ad-4bc1-9b25-5a0f80bb3b42",
   "metadata": {},
   "source": [
    "*   **Goal**: To tokenize every review in the dataset and add the resulting `input_ids` and `attention_mask` as new columns directly to the dataset object itself, all in one efficient operation.\n",
    "\n",
    "*   **Method**:\n",
    "    1.  **`ds.map(...)`**: This is the core function. It's designed to apply any function to the dataset's examples.\n",
    "    2.  **`lambda batch: tokenizer(batch[\"text\"])`**: This is the function we are applying. It's a short, anonymous `lambda` function that takes a batch of examples, accesses the `text` column (which is a list of strings), and passes it to our `tokenizer`.\n",
    "    3.  **`batched=True`**: This is the key to its speed. It tells `.map()` to send data to our lambda function in batches rather than one example at a time. This allows the tokenizer to leverage its internal parallel processing capabilities, making the overall process much faster.\n",
    "\n",
    "*   **Result**:\n",
    "    *   The output `t` is a new `Dataset` object. It contains all the original columns (`text`, `label`) **plus** the new columns generated by the tokenizer: `input_ids` and `attention_mask`.\n",
    "    *    The exact same process is then applied to the `validation` and `test` splits to create their corresponding tokenized datasets (e.g., `v` and `test`), ensuring all data is processed consistently.\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b2ecd2f-30f9-457f-9b8c-8f05b3c36bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\", split =\"train\")\n",
    "ds.set_format(type=\"torch\" , columns = [\"text\", \"label\"])\n",
    "t = ds.map(lambda batch: tokenizer(batch[\"text\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1489951f-3a8a-4dff-af91-15d1aebf0ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\", split =\"validation\")\n",
    "ds.set_format(type=\"torch\" , columns = [\"text\", \"label\"])\n",
    "v = ds.map(lambda batch: tokenizer(batch[\"text\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be648f1a-27bd-4df7-9b62-e2f0de46308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\", split =\"test\")\n",
    "ds.set_format(type=\"torch\" , columns = [\"text\", \"label\"])\n",
    "test = ds.map(lambda batch: tokenizer(batch[\"text\"]), batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c674901-595a-425e-89a8-659ca26a8fb3",
   "metadata": {},
   "source": [
    "\n",
    "#### Exercise 2.2: Setting up the Model to be Fine-tuned\n",
    "\n",
    "In this exercise we need to prepare the base Distilbert model for fine-tuning for a *sequence classification task*. This means, at the very least, appending a new, randomly-initialized classification head connected to the `[CLS]` token of the last transformer layer. Luckily, HuggingFace already provides an `AutoModel` for just this type of instantiation: [`AutoModelForSequenceClassification`](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodelforsequenceclassification). You will want you instantiate one of these for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f543a7-1892-4032-8985-d341da68146d",
   "metadata": {},
   "source": [
    "*   **Goal**: To load a pre-trained model architecture that includes a classification \"head\" on top of the base DistilBERT layers. This creates a single, integrated model that can be trained end-to-end for our specific sentiment analysis task.\n",
    "\n",
    "*   **Method**:\n",
    "    *   We use `AutoModelForSequenceClassification.from_pretrained(...)` instead of the more generic `AutoModel`.\n",
    "    *   This class automatically downloads the standard DistilBERT weights and then appends a new, **randomly initialized** classification layer on top.\n",
    "    *   You will likely see a warning message saying `Some weights of DistilBertForSequenceClassification were not initialized...`. This is **expected and correct!** It's telling you that while the main body of the model is pre-trained, the new classification head is like a blank slate, ready for you to train on your dataset.\n",
    "\n",
    "*   **Result**:\n",
    "    *   When you display the `model` object, you will see the familiar DistilBERT architecture, but at the very end, you will find two new layers: a `pre_classifier` and a `classifier`.\n",
    "    *   The `classifier` layer is a simple linear layer that will take the final representation from the base model and output raw prediction scores (logits) for each class (in this case, for \"negative\" and \"positive\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f8dbbfcd-780f-497b-a7e0-6cffb9831e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "706817a2-a379-4eb5-b446-cd3f8b677dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d8c9f3-1637-48ab-9f6a-c2c0cc4d6caf",
   "metadata": {},
   "source": [
    "\n",
    "#### Exercise 2.3: Fine-tuning Distilbert\n",
    "\n",
    "Finally. In this exercise you should use a HuggingFace [`Trainer`](https://huggingface.co/docs/transformers/main/en/trainer) to fine-tune your model on the Rotten Tomatoes training split. Setting up the trainer will involve (at least):\n",
    "\n",
    "\n",
    "1. Instantiating a [`DataCollatorWithPadding`](https://huggingface.co/docs/transformers/en/main_classes/data_collator) object which is what *actually* does your batch construction (by padding all sequences to the same length).\n",
    "2. Writing an *evaluation function* that will measure the classification accuracy. This function takes a single argument which is a tuple containing `(logits, labels)` which you should use to compute classification accuracy (and maybe other metrics like F1 score, precision, recall) and return a `dict` with these metrics.  \n",
    "3. Instantiating a [`TrainingArguments`](https://huggingface.co/docs/transformers/v4.51.1/en/main_classes/trainer#transformers.TrainingArguments) object using some reasonable defaults.\n",
    "4. Instantiating a `Trainer` object using your train and validation splits, you data collator, and function to compute performance metrics.\n",
    "5. Calling `trainer.train()`, waiting, waiting some more, and then calling `trainer.evaluate()` to see how it did.\n",
    "\n",
    "**Tip**: When prototyping this laboratory I discovered the HuggingFace [Evaluate library](https://huggingface.co/docs/evaluate/en/index) which provides evaluation metrics. However I found it to have insufferable layers of abstraction and getting actual metrics computed. I suggest just using the Scikit-learn metrics..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0e33fbcd-0feb-4b70-9850-0ea3772eaa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.53.0 1.9.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23474' max='533500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 23474/533500 16:32 < 5:59:36, 23.64 it/s, Epoch 22/500]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.672200</td>\n",
       "      <td>0.601514</td>\n",
       "      <td>0.774859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.510800</td>\n",
       "      <td>0.450714</td>\n",
       "      <td>0.804878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.427700</td>\n",
       "      <td>0.418276</td>\n",
       "      <td>0.816135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.394600</td>\n",
       "      <td>0.404644</td>\n",
       "      <td>0.829268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.375100</td>\n",
       "      <td>0.400221</td>\n",
       "      <td>0.833021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.361100</td>\n",
       "      <td>0.400364</td>\n",
       "      <td>0.838649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.350100</td>\n",
       "      <td>0.399989</td>\n",
       "      <td>0.839587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.338200</td>\n",
       "      <td>0.405790</td>\n",
       "      <td>0.842402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.328100</td>\n",
       "      <td>0.419834</td>\n",
       "      <td>0.839587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.321700</td>\n",
       "      <td>0.428557</td>\n",
       "      <td>0.842402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.316000</td>\n",
       "      <td>0.431154</td>\n",
       "      <td>0.847092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.306800</td>\n",
       "      <td>0.437259</td>\n",
       "      <td>0.848030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.302200</td>\n",
       "      <td>0.452786</td>\n",
       "      <td>0.843340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.464925</td>\n",
       "      <td>0.841463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.287700</td>\n",
       "      <td>0.473942</td>\n",
       "      <td>0.843340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.277300</td>\n",
       "      <td>0.484100</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.268200</td>\n",
       "      <td>0.496488</td>\n",
       "      <td>0.842402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.265600</td>\n",
       "      <td>0.509231</td>\n",
       "      <td>0.845216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.257600</td>\n",
       "      <td>0.526096</td>\n",
       "      <td>0.844278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.257200</td>\n",
       "      <td>0.532162</td>\n",
       "      <td>0.848030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.239900</td>\n",
       "      <td>0.544748</td>\n",
       "      <td>0.847092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.239900</td>\n",
       "      <td>0.553926</td>\n",
       "      <td>0.847092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=23474, training_loss=0.3358169182547213, metrics={'train_runtime': 993.1934, 'train_samples_per_second': 4294.229, 'train_steps_per_second': 537.156, 'total_flos': 2156437300171800.0, 'train_loss': 0.3358169182547213, 'epoch': 22.0})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import evaluate\n",
    "import accelerate\n",
    "import copy\n",
    "import transformers\n",
    "from transformers import EarlyStoppingCallback\n",
    "print(transformers.__version__, accelerate.__version__)\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "from datetime import datetime\n",
    "\n",
    "run_name = f\"distillBert_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./result/{run_name}\",           \n",
    "    num_train_epochs=500,\n",
    "    logging_dir=f\"./log/{run_name}\",            \n",
    "    report_to=\"tensorboard\",\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    load_best_model_at_end=True,\n",
    "    eval_strategy=\"epoch\",  \n",
    "    save_strategy = \"epoch\", \n",
    "    save_total_limit=2,\n",
    "    logging_strategy = \"epoch\",\n",
    "    weight_decay=0.01,\n",
    "    learning_rate = 5e-7\n",
    "    \n",
    ")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=copy.deepcopy(model),\n",
    "    args=training_args,\n",
    "    train_dataset=t,\n",
    "    eval_dataset=v,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)] \n",
    ")\n",
    "trainer.train()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2abe2657-df0c-4030-8fd6-ad225d50b890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='268' max='134' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [134/134 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.530176043510437,\n",
       " 'eval_accuracy': 0.849906191369606,\n",
       " 'eval_runtime': 1.29,\n",
       " 'eval_samples_per_second': 826.355,\n",
       " 'eval_steps_per_second': 103.876,\n",
       " 'epoch': 12.0}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "557db500-5c01-42e4-816c-f53c8726a387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='134' max='134' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [134/134 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.47394126653671265,\n",
       " 'eval_accuracy': 0.8377110694183865,\n",
       " 'eval_runtime': 1.357,\n",
       " 'eval_samples_per_second': 785.557,\n",
       " 'eval_steps_per_second': 98.747,\n",
       " 'epoch': 22.0}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7b2ed5-4eee-4007-9087-6b3735511dd1",
   "metadata": {},
   "source": [
    "# Analysis of the Results\n",
    "A comparison of the two methods for leveraging DistilBERT highlights a classic trade-off between computational cost and final performance. The fully fine-tuned model achieved a test accuracy of 84.33%, outperforming the 80.68% score of the DistilBERT-plus-SVM feature extraction approach. This ~3.7% accuracy gain stems directly from adapting all of the model’s weights to the specific characteristics of the movie review sentiment task.\n",
    "\n",
    "However, the feature extraction method proves its worth by delivering strong and consistent results while requiring significantly less computational time and resources. This makes it an excellent choice for a fast, dependable baseline or for projects with limited GPU availability, whereas full fine-tuning remains the preferred option when maximizing predictive accuracy is the top priority.\n",
    "\n",
    "Notably, during fine-tuning, we observed an intriguing phenomenon: test accuracy continued to rise even after training loss indicated overfitting. This well-known divergence occurs because cross-entropy loss and accuracy, while correlated, measure different things. As the model becomes overconfident, its predictions grow more polarized. While this often increases the average number of correct predictions (boosting accuracy), it also amplifies the magnitude of occasional large mistakes due to misplaced confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c44cfaf-e576-4f8d-9ab0-99e966b366a4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Exercise 3.2: Fine-tuning a CLIP Model (harder)\n",
    "\n",
    "Use a (small) CLIP model like [`openai/clip-vit-base-patch16`](https://huggingface.co/openai/clip-vit-base-patch16) and evaluate its zero-shot performance on a small image classification dataset like ImageNette or TinyImageNet. Fine-tune (using a parameter-efficient method!) the CLIP model to see how much improvement you can squeeze out of it.\n",
    "\n",
    "**Note**: There are several ways to adapt the CLIP model; you could fine-tune the image encoder, the text encoder, or both. Or, you could experiment with prompt learning.\n",
    "\n",
    "**Tip**: CLIP probably already works very well on ImageNet and ImageNet-like images. For extra fun, look for an image classification dataset with different image types (e.g. *sketches*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eb811727fa9833",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# CoOp\n",
    "leggendo la richiesta sopra mi sono subito interessato all prompt learaning, in particolare al metodo [`CoOp`](https://arxiv.org/pdf/2109.01134) ho iniziato verificando come il modello genera le text features. Ho confrontato le embedding testuali ottenute in modo standard con quelle calcolate manualmente, ricostruendo passo passo l'intero flusso: partendo dagli embedding di token e posizione, passando per l'encoder transformer con la casual attention mask, applicando la layer norm finale e selezionando il token [EOT], fino alla proiezione lineare conclusiva. Il confronto ha mostrato che le text features manuali sono identiche a quelle standard, così come i logits ottenuti dal prodotto scalare con le image features. Questa verifica è fondamentale per poter in seguito sostituire gli embedding statici con quelli apprendibili, come previsto dal metodo CoOp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bc34a2-b8b6-4788-8147-4648c06c0557",
   "metadata": {},
   "source": [
    "The previous discussion got me interested in prompt learning, and I decided to look into the method  [`CoOp`](https://arxiv.org/pdf/2109.01134) . My first step here is to break down the most basic part of the process: how the model turns a text prompt into its corresponding feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c1f07a0-462d-4599-9e01-2fd08ad507b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "      (position_embedding): Embedding(197, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from transformers.modeling_attn_mask_utils import _create_4d_causal_attention_mask\n",
    "\n",
    "model_name = \"openai/clip-vit-base-patch16\"\n",
    "model = CLIPModel.from_pretrained(model_name).eval()\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "texts = [\"a photo of a cat\", \"a photo of a dog\"]\n",
    "inputs = processor(text=texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "input_ids = inputs['input_ids']\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1336fa-3181-4c2a-8803-92fc7cdc7467",
   "metadata": {},
   "source": [
    "*   **Goal**: To prove that the initial input vector for the Transformer layers is the element-wise sum of its **token embeddings**  and its **positional embeddings** .\n",
    "*   **Method**:\n",
    "    1.  **Run the Model**: We first run the `text_model` with `output_hidden_states=True`. This gives us access to the output of every layer. The very first hidden state, `outputs.hidden_states[0]`, is the embedding *after* the token and positional information has been combined. This is our \"ground truth\".\n",
    "    2.  **Isolate the Embedding Layer**: We get a direct reference to the `embeddings` module of the text model.\n",
    "    3.  **Get Embedding Components**: We manually call the two sub-layers:\n",
    "        *   `token_embedding`: Converts the numerical `input_ids` into dense vectors.\n",
    "        *   `position_embedding`: Creates a unique vector for each position in the sequence (0, 1, 2, ...).\n",
    "    4.  **Combine the Embeddings**: We simply add (`+`) the token and positional embeddings together. This combines the \"what\" information (the token) with the \"where\" information (the position).\n",
    "    5.  **Verification**: We use `torch.equal()` to perform a strict, element-by-element comparison between our manually created tensor (`manual_initial_embeddings`) and the model's first hidden state.\n",
    "  *   **Result**:\n",
    "      *   The code will print `gli emebendings sono uguali?: True`.\n",
    "      *   This confirms our understanding of **how the initial text embeddings are created** in a Transformer-based model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae9548f9-2553-44a7-8c20-1c4ca4b4a764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 512])\n",
      "torch.Size([2, 7, 512])\n",
      "gli embending sono uguali? True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    outputs = model.text_model(**inputs, output_hidden_states=True)\n",
    "    standard_text_features = model.get_text_features(input_ids=input_ids)\n",
    "\n",
    "\n",
    "    embeddings_module = model.text_model.embeddings\n",
    "    token_embeds = embeddings_module.token_embedding(input_ids)\n",
    "    position_ids = torch.arange(input_ids.shape[1], device=device).unsqueeze(0)\n",
    "    positional_embeds = embeddings_module.position_embedding(position_ids)\n",
    "    manual_initial_embeddings = token_embeds + positional_embeds\n",
    "    \n",
    "    print(\"gli embending sono uguali?\", manual_initial_embeddings.equal(outputs.hidden_states[0]))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d26897-c293-424f-ae3f-f4b477b25cdf",
   "metadata": {},
   "source": [
    "*   **Goal**: To manually replicate the entire text feature extraction process after the initial embedding step, confirming a correct understanding of the encoder, pooling, and projection stages.\n",
    "\n",
    "*   **Method**: This is a multi-stage process that mirrors the model's internal source code.\n",
    "    1.  **Causal Attention Mask**: The code first creates the specific `causal_attention_mask` required by the encoder. CLIP's text model is autoregressive (meaning it processes text sequentially), and this mask enforces the rule that a token can only attend to itself and previous tokens.\n",
    "    2.  **Transformer Encoder**: The code then feeds the `manual_initial_embeddings` and the `causal_attention_mask` directly into the `encoder` module. This module contains the stack of Transformer layers that processes the embeddings to produce the final contextualized hidden states.\n",
    "    3.  **Final Feature Recreation**: This three-part process replicates exactly what `model.get_text_features()` does internally:\n",
    "        *   **A. Final Layer Norm**: The output of the last encoder layer is passed through one final `LayerNorm`.\n",
    "        *   **B. Find EOT Token**: CLIP's pooling strategy uses the output vector corresponding to the special `<|endoftext|>` token. The code finds the position of this token in each sentence.\n",
    "        *   **C. Pooling**: It then selects only the hidden states from these EOT token positions, resulting in a single representative vector for each sentence.\n",
    "        *   **D. Projection**: This final vector is passed through one last linear layer (`text_projection`) to map it into the final, shared multi-modal embedding space.\n",
    "    4.  **Verification**: The manually created `manual_text_features` are compared against the `standard_text_features` obtained from the simple, high-level API call.\n",
    "\n",
    "*   **Result**:\n",
    "    *   he code will print`le features sono uguali?` `True`.\n",
    "    *   This result is a powerful confirmation, proving that the manual implementation correctly replicates the **entire, end-to-end CLIP text encoding pipeline**. It validates a correct understanding of how the model enforces causality, how it pools information from the `<|endoftext|>` token, and how it projects the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac9ebb3c-49c6-4376-b4cf-e6d1ae92fbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l'uscita dell'encoder è uguale : 12\n",
      "features sono uguali? True\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "        # per utilizzare correttamente l'encoder è fondamentale creare una maschera di attenzione causale ho seguito l'implementazione a \n",
    "        # https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/modeling_clip.py#L617 \n",
    "    causal_attention_mask = _create_4d_causal_attention_mask(\n",
    "        input_ids.size(), manual_initial_embeddings.dtype, device=device\n",
    "    )  \n",
    "    encoder_outputs = model.text_model.encoder(inputs_embeds=manual_initial_embeddings, \n",
    "                                               output_hidden_states=True , \n",
    "                                               causal_attention_mask=causal_attention_mask)\n",
    "    i = 0 \n",
    "    for hidden in outputs.hidden_states:\n",
    "        if encoder_outputs.last_hidden_state.equal(hidden):\n",
    "            print(\"l'uscita dell'encoder è uguale :\", i)\n",
    "        i+=1\n",
    "        \n",
    "    standard_text_features = model.get_text_features(input_ids=input_ids, output_hidden_states=True )\n",
    "  \n",
    "    last_hidden_state = encoder_outputs.last_hidden_state\n",
    "    normed_hidden_state = model.text_model.final_layer_norm(last_hidden_state)\n",
    "    eot_token_positions = torch.argmax(input_ids, dim=-1)\n",
    "    pooled_output = normed_hidden_state[torch.arange(normed_hidden_state.shape[0], device=device), eot_token_positions] #prendo eot da ogni riga \n",
    "    manual_text_features = model.text_projection(pooled_output)\n",
    "    print(\"features sono uguali?\",standard_text_features.equal(manual_text_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba519400-1b91-4d6d-9856-a062cba07a51",
   "metadata": {},
   "source": [
    "*   **Goal**: To prove that the final logits (similarity scores) between an image and a set of text prompts are the result of a scaled cosine similarity between their respective L2-normalized feature vectors.\n",
    "\n",
    "*   **Method**: The code follows two parallel paths to compute the same result.\n",
    "    1.  **Manual Calculation**: This path explicitly performs the steps at the core of contrastive learning.\n",
    "        *   First, it gets the features for the input image.\n",
    "        *   Crucially, it **L2-normalizes** both the image features and the `manual_text_features` (from the previous step). This projects both feature vectors onto a unit hypersphere, ensuring that their dot product is equivalent to their cosine similarity.\n",
    "        *   It retrieves the model's `logit_scale`, a learnable parameter that scales the similarities before the final softmax.\n",
    "        *   Finally, it computes the scaled dot product (`logit_scale * image_features_norm @ text_features_norm_man.t()`). This matrix multiplication effectively calculates the cosine similarity between the single image vector and the multiple text vectors.\n",
    "    2.  **Standard API Calculation**: This is the easy, high-level way.\n",
    "        *   The code uses the `processor` to prepare both the image and text prompts at once.\n",
    "        *   It passes this combined input to the `model` and directly retrieves the final `logits_per_image` from the output object. This single call hides all the normalization and matrix multiplication steps that were done manually above.\n",
    "    3.  **Verification**: The code concludes by printing the logits from both methods, allowing for a direct comparison.\n",
    "\n",
    "*   **Result**:\n",
    "    *   The two printed arrays of logits will be identical (or extremely close due to floating-point precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T14:37:51.203341200Z",
     "start_time": "2025-07-02T14:37:35.810341700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- VERIFICA DEI RISULTATI FINALI (LOGITS) ---\n",
      "Logits calcolati con metodo STANDARD: [[22.73867  15.661786]]\n",
      "Logits calcolati con metodo MANUALE:  [[22.738672 15.661789]]\n"
     ]
    }
   ],
   "source": [
    "   with torch.no_grad():\n",
    "       \n",
    "    image_inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    image_features = model.get_image_features(**image_inputs)\n",
    "\n",
    "       \n",
    "    #https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/modeling_clip.py#L1026 \n",
    "    #ripreso da 1026 a 1035\n",
    "    image_features_norm = image_features / image_features.norm(p=2, dim=-1, keepdim=True) # implementazione in una riga di vector norm\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "  \n",
    "    text_features_norm_man = manual_text_features / manual_text_features.norm(p=2, dim=-1, keepdim=True)\n",
    "    logits_manual = logit_scale * image_features_norm @ text_features_norm_man.t()\n",
    "     #al posto di mat mul ho usato @ \n",
    "    \n",
    "     \n",
    "    inputs_image_text = processor(text=texts, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "    standard_outputs = model(**inputs_image_text)\n",
    "    logits_standard = standard_outputs.logits_per_image\n",
    "    \n",
    "        \n",
    "    print(\"--- VERIFICA DEI RISULTATI FINALI (LOGITS) ---\")\n",
    "    print(\"Logits calcolati con metodo STANDARD:\", logits_standard.cpu().numpy())\n",
    "    print(\"Logits calcolati con metodo MANUALE: \", logits_manual.cpu().numpy())\n",
    "    \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c033b6078062f0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Define CoOp\n",
    "\n",
    "*   **Goal**: To adapt a large, **frozen**, pre-trained CLIP model for a specific image classification task in a highly parameter-efficient way. Instead of fine-tuning the entire CLIP model, this class learns only a small set of `context_vectors` that act as a universal prompt for all classes.\n",
    "\n",
    "*   **Method**:\n",
    "    1.  **Initialization (`__init__`)**: The process begins by creating the learnable prompt. A tensor named `ctx_vectors` is initialized with random values. This tensor is then wrapped in `torch.nn.Parameter`, making `self.context_vectors` the one and only set of trainable parameters in the entire class. Immediately after, a loop iterates through the entire `self.clip_model` and sets `param.requires_grad = False` for every parameter, effectively freezing the massive CLIP model.\n",
    "    2.  **Prompt Construction (`construct_prompts`)**: This method builds the prompts that will be fed to the text encoder. It retrieves the static embeddings for the start-of-sequence token and the `class_names` (e.g., \"dog\", \"cat\"). It then concatenates these with the learnable `self.context_vectors` to form a complete prompt embedding for each class (e.g., `[SOS_embedding] + [learned_context] + [class_name_embedding] +[EOT_embedding]`).\n",
    "    3.  **Forward Pass (`forward`)**: When an `image` is passed to the model:\n",
    "        *   The image is processed by the frozen `clip_model.get_image_features()` to produce a single `image_features` vector.\n",
    "        *   In parallel, `construct_prompts()` is called to get the prompt embeddings for all classes, using the current state of the learnable `self.context_vectors`.\n",
    "        *   These prompt embeddings are passed through the frozen `clip_model.text_model` to produce a final `text_features` vector for each class.\n",
    "        *   Finally, the model calculates the similarity (as `logits`) between the `image_features` and all of the `text_features` via a matrix multiplication (`image_features @ text_features.t()`).\n",
    "\n",
    "*   **Result**:\n",
    "    *   The output is a tensor of `logits` of shape `[batch_size, num_classes]`, where each value represents the similarity between the input image and a class as described by the learned prompt.\n",
    "    *   Crucially, because only `self.context_vectors` has `requires_grad=True`, when a loss is calculated and `.backward()` is called during a training loop, gradients will *only* flow back to and update these context vectors. The model effectively learns the optimal \"words\" to use in a prompt for the given task, without ever touching the weights of the original CLIP model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4972776f8329c8dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T14:37:51.217340300Z",
     "start_time": "2025-07-02T14:37:51.209341100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
    "from transformers.modeling_attn_mask_utils import _create_4d_causal_attention_mask\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_NAME = \"openai/clip-vit-base-patch16\"\n",
    "\n",
    "class CoOpCLIP(nn.Module):\n",
    "    def __init__(self, class_names, clip_model, m_ctx=16):\n",
    "        super().__init__()\n",
    "        self.class_names = class_names\n",
    "        self.n_classes = len(class_names)\n",
    "        self.n_ctx = m_ctx\n",
    "        self.clip_model = clip_model\n",
    "        embedding_dim = clip_model.text_model.config.hidden_size\n",
    "        ctx_vectors = torch.empty(self.n_ctx, embedding_dim, device=DEVICE)\n",
    "        nn.init.normal_(ctx_vectors, std=0.02)\n",
    "        self.context_vectors = nn.Parameter(ctx_vectors)\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(MODEL_NAME)\n",
    "        tokenized_classes = self.tokenizer([c for c in class_names], padding = True, return_tensors=\"pt\")\n",
    "        self.class_token_ids = tokenized_classes.input_ids.to(DEVICE)\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def construct_prompts(self):\n",
    "        embedding_layer = self.clip_model.text_model.embeddings.token_embedding\n",
    "        prefix_embeddings = embedding_layer(self.class_token_ids[:, :1])\n",
    "        class_embeddings = embedding_layer(self.class_token_ids[:, 1:])\n",
    "        context_embeddings = self.context_vectors.unsqueeze(0).expand(self.n_classes, -1, -1)  #unified context\n",
    "        prompt_embeddings = torch.cat([prefix_embeddings, context_embeddings, class_embeddings], dim=1)\n",
    "        return prompt_embeddings\n",
    "\n",
    "    def forward(self, image):\n",
    "       \n",
    "\n",
    "        prompt_embeds = self.construct_prompts()\n",
    "        \n",
    "        text_encoder = self.clip_model.text_model\n",
    "      \n",
    "        pos_ids = torch.arange(prompt_embeds.size(1), device=DEVICE).unsqueeze(0)\n",
    "        positional_embeddings = text_encoder.embeddings.position_embedding(pos_ids)\n",
    "        inputs_embeds = prompt_embeds\n",
    "      #  print(positional_embeddings.shape)\n",
    "       # print(inputs_embeds.shape)\n",
    "        inputs_embeds[:, 0] += positional_embeddings[:, 0]\n",
    "        inputs_embeds[:, 16:] +=  positional_embeddings[:, 16:]\n",
    "\n",
    "        \n",
    "        input_shape = prompt_embeds.shape[:2]\n",
    "        # la forma degli input_ids  [102 , 1+16+7 = 24]  [nclassi, prefisso + context + class]\n",
    "        \n",
    "        causal_attention_mask = _create_4d_causal_attention_mask(\n",
    "            input_shape, inputs_embeds.dtype, device=DEVICE\n",
    "        )\n",
    "\n",
    "        encoder_outputs = text_encoder.encoder(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            causal_attention_mask=causal_attention_mask\n",
    "        )\n",
    "        last_hidden_state = encoder_outputs.last_hidden_state\n",
    "\n",
    "        eos_positions = self.class_token_ids.argmax(dim=-1) \n",
    "      \n",
    "        final_token_indices = self.n_ctx + (eos_positions)\n",
    "\n",
    "        pooled_output = last_hidden_state[torch.arange(self.n_classes), final_token_indices]\n",
    " \n",
    "        pooled_output = text_encoder.final_layer_norm(pooled_output)\n",
    "        \n",
    "        \n",
    "        \n",
    "        image_features = self.clip_model.get_image_features(pixel_values=image)\n",
    "        image_features = image_features / image_features.norm(p=2,dim=-1, keepdim=True)\n",
    "        \n",
    "        text_features = self.clip_model.text_projection(pooled_output)\n",
    "        text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)  \n",
    "        \n",
    "        logit_scale = self.clip_model.logit_scale.exp()\n",
    "       \n",
    "        logits = logit_scale * image_features @ text_features.t()\n",
    "       \n",
    "\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66b196a1d379934",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Few Shot Dataset\n",
    "*   **Goal**: To create a small, **class-balanced** dataset suitable for few-shot learning experiments. This means selecting exactly `num_shots` examples from each category.\n",
    "\n",
    "*   **Method**:\n",
    "    1.  **Group Indices by Class**: The function first iterates through the entire dataset to build a dictionary (`samples_per_class`) that maps each class label to a list of all the indices of the samples belonging to that class.\n",
    "    2.  **Sample from Each Class**: It then loops through this dictionary. For each class, it uses `random.sample()` to randomly select `num_shots` unique indices from the list. This ensures that every class is represented equally in the final subset.\n",
    "    3.  **Shuffle Indices**: After collecting the chosen indices, the list is shuffled. This is important to ensure that when a `DataLoader` iterates over this data, it receives batches with a random mix of classes, which is better for model training.\n",
    "    4.  **Create a `Subset`**: Finally, it returns a `torch.utils.data.Subset` object. This is a highly efficient PyTorch class that wraps the original dataset but only exposes the items specified in the `selected_indices` list. It doesn't duplicate any data, making it very memory-friendly.\n",
    "\n",
    "*   **Result**:\n",
    "    *   The function returns a new dataset object (a `Subset`) that is small and perfectly balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a4f38bb3-bfe2-4c20-a036-2580dd74537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "def create_few_shot_split(full_dataset, num_shots):\n",
    "        samples_per_class = {}\n",
    "        clasees =  full_dataset.classes\n",
    "        for idx, (_, label) in enumerate(full_dataset):\n",
    "            if label not in samples_per_class:\n",
    "                samples_per_class[label] = []\n",
    "            samples_per_class[label].append(idx)\n",
    "        selected_indices = []\n",
    "        for label, indices in samples_per_class.items():\n",
    "            chosen = random.sample(indices, min(num_shots, len(indices)))\n",
    "            selected_indices.extend(chosen)\n",
    "        random.shuffle(selected_indices)\n",
    "        return Subset(full_dataset, selected_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c864db65ced403",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Accuracy\n",
    "*   **Goal**: To provide a single function that measure a model's accuracy on a given dataset. It uses its parameters to switch between a standard evaluation and a zero-shot evaluation.\n",
    "\n",
    "*   **Method**: The function's logic is controlled by its input parameters.\n",
    "\n",
    "    1.  **Understanding the Parameters**:\n",
    "        *   `model`: The trained PyTorch model to be evaluated. It's the core component that will make the predictions.\n",
    "        *   `dataloader`: A PyTorch `DataLoader` that supplies batches of images and their true labels from the evaluation dataset.\n",
    "        *   `processor`: A Hugging Face `processor` object (like `CLIPProcessor`) that handles the model-specific text tokenization. This is **only used in zero-shot mode**.\n",
    "        *   `class_names`: A list of strings representing the names of the classes (e.g., `['rose', 'daisy']`). This is also **only used in zero-shot mode** to create the text prompts.\n",
    "        *   `is_zeroshot`: A boolean flag that acts as a switch. If `True`, the function runs zero-shot logic; otherwise, it runs standard logic.\n",
    "\n",
    "    2.  **Execution Flow**:\n",
    "        *   The function first sets the `model` to evaluation mode (`model.eval()`) and disables gradient calculations (`@torch.no_grad()`) for efficiency.\n",
    "        *   **If `is_zeroshot` is `True`**: It uses the `processor` and `class_names` to create and encode text prompts (e.g., \"a photo of a rose\") into feature vectors. This happens only once.\n",
    "        *   **Main Loop**: It iterates through the `dataloader` to get batches of `images` and `labels`.\n",
    "        *   Inside the loop, if in **zero-shot mode**, it calculates the similarity between each image's features and the pre-computed text features to get a prediction. If in **standard mode**, it simply feeds the `images` to the `model` to get a direct prediction.\n",
    "        *   The predictions and true labels for each batch are collected by the `accuracy_metric` object.\n",
    "\n",
    "*   **Result**:\n",
    "    *   It returns this single accuracy score as a percentage (e.g., `85.5`), providing a measure of the `model`'s performance on the data supplied by the `dataloader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3dc1a12a056a4a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T14:37:51.726341500Z",
     "start_time": "2025-07-02T14:37:51.231341600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import evaluate as hfevaluate\n",
    "from tqdm.notebook import tqdm\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, processor, class_names, is_zeroshot=False):\n",
    "    model.eval()\n",
    "    accuracy_metric = hfevaluate.load(\"accuracy\")\n",
    "    if is_zeroshot:\n",
    "        text_inputs = processor(\n",
    "            text=[f\"a photo of a {c}, a type of flower.\" for c in class_names],  #da fig 1 pag 2 si osserva che si ottiene risultati migliori \n",
    "            return_tensors=\"pt\", padding=True\n",
    "        ).to(DEVICE)\n",
    "        text_features = model.get_text_features(**text_inputs)\n",
    "        text_features /= text_features.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "    pbar = tqdm(dataloader, desc=\"Valutazione\")\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        if is_zeroshot:\n",
    "            image_features = model.get_image_features(pixel_values=images)\n",
    "            image_features /= image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "            logits = model.logit_scale.exp() * image_features @ text_features.t()\n",
    "        else: \n",
    "            logits = model(images)\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        accuracy_metric.add_batch(predictions=predictions, references=labels)\n",
    "    results = accuracy_metric.compute()\n",
    "    return results['accuracy'] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2a48f0a070918c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Train and Evaluate\n",
    "\n",
    "This script is a complete machine learning experiment designed to replicate the findings of the paper [**\"Learning to Prompt for Vision-Language Models\"**](https://arxiv.org/pdf/2109.01134). It tests the paper's central hypothesis: that learning a \"prompt\" for a vision-language model like CLIP can significantly outperform standard, handcrafted prompts, especially in a data-scarce, few-shot learning context.\n",
    "\n",
    "#### Goal\n",
    "The primary goal is to scientifically validate the effectiveness of the CoOp method by comparing its performance against a standard zero-shot CLIP baseline on an image classification task (Flowers102). The experiment aims to demonstrate that by training only a small set of prompt vectors, one can achieve superior accuracy with very few labeled examples (e.g., 1, 2, 4, or 8 shots per class).\n",
    "\n",
    "#### Method\n",
    "1.  **Establish Baseline**: First, the script establishes a performance baseline by evaluating an off-the-shelf, pre-trained CLIP model on the Flowers102 test set in a **zero-shot** manner. This involves using fixed, handcrafted text prompts (e.g., \"a flower a photo of a [CLASS]\") to measure the model's out-of-the-box accuracy. This score serves as the benchmark to beat.\n",
    "2.  **Implement CoOp Methodology**: The script then implements the CoOp training procedure as described in the peaper. It iterates through a series of few-shot scenarios (`[1, 2, 4, 8]` shots). In each scenario:\n",
    "    *   A tiny, class-balanced training subset is created from the full training data.\n",
    "    *   A special `CoOpCLIP` model is instantiated. This model contains a small number of learnable context vectors (a \"learnable prompt\") while the  CLIP vision and text encoders remain **frozen**.\n",
    "    *   An optimizer is configured to train **only** these \"learnable prompt\" vectors on the few-shot data.\n",
    "    *   The model is trained, and its performance is periodically checked against a validation set. The version of the model with the highest validation accuracy is saved.\n",
    "3.  **Final Evaluation**: After training is complete for a given few-shot scenario, the best-saved model is loaded and its final performance is measured on the unseen test set.\n",
    "\n",
    "#### Result\n",
    "The script produces two key outputs that allow for a direct, quantitative comparison:\n",
    "1.  A single accuracy score for the **zero-shot CLIP baseline**.\n",
    "2.  A set of final accuracy scores for the **CoOp models**, one for each few-shot training scenario.\n",
    "\n",
    "By comparing these results, the script demonstrates the core finding of the paper: the CoOp models, whose prompts have been optimized on as few as 1-8 images per class, consistently and significantly outperform the generic zero-shot model. The final output explicitly shows the performance gain of CoOp over the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "532fb316698316e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:28:43.159918600Z",
     "start_time": "2025-07-02T14:37:51.730340100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricato il dataset Flowers102 con 1020 immagini di addestramento, 6149 immagini di test e 1020 immagini di validazione.\n",
      "\n",
      "--- Calcolo della Baseline Zero-Shot CLIP ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "000b1f31c28e4a5aab4c4e290f63631f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Zero-Shot CLIP: 71.30%\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Addestramento CoOp con 1 shot per classe ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e68139b501e4e95b18db8dcac8463a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Addestramento CoOp (1-shot):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d93919ae434369b98246f454f03f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Loss: 0.2445\n",
      "Validation Accuracy: 80.29%\n",
      " Nuovo modello migliore salvato in 'best_models/best_coop_1-shot.pth' con accuracy 80.29%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9907190112ff4d2ca54c73d718938cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Loss: 0.0607\n",
      "Validation Accuracy: 81.86%\n",
      " Nuovo modello migliore salvato in 'best_models/best_coop_1-shot.pth' con accuracy 81.86%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245f3a373fb245049e26c40d66ed29d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Loss: 0.0396\n",
      "Validation Accuracy: 82.06%\n",
      " Nuovo modello migliore salvato in 'best_models/best_coop_1-shot.pth' con accuracy 82.06%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659201f79375437da1bd5b7029ee3f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, Loss: 0.0333\n",
      "Validation Accuracy: 81.67%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db08ffcc950443b89fb9a1bd05944418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Loss: 0.0334\n",
      "Validation Accuracy: 81.67%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa811df598543568535f3b86b0e897f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy CoOp (1-shot) sul TEST SET: 79.70%\n",
      "Differenza vs Zero-Shot: +8.41%\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Addestramento CoOp con 2 shot per classe ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f90a57e73de4c488d1ec6c10be9db89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Addestramento CoOp (2-shot):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc04c0bee5624fdabfbc37bab4247ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 0.3004\n",
      "Validation Accuracy: 85.78%\n",
      " Nuovo modello migliore salvato in 'best_models/best_coop_2-shot.pth' con accuracy 85.78%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c675e89ec030452da2e4cf587269a0dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100, Loss: 0.0507\n",
      "Validation Accuracy: 88.82%\n",
      " Nuovo modello migliore salvato in 'best_models/best_coop_2-shot.pth' con accuracy 88.82%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cafa1aa5301430fb7fdafa147adfa5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100, Loss: 0.0281\n",
      "Validation Accuracy: 88.33%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05aed17e090e454fac8df17181ad240c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100, Loss: 0.0206\n",
      "Validation Accuracy: 89.41%\n",
      " Nuovo modello migliore salvato in 'best_models/best_coop_2-shot.pth' con accuracy 89.41%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c070e6d88740a0a6c811af717f34af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100, Loss: 0.0167\n",
      "Validation Accuracy: 89.51%\n",
      " Nuovo modello migliore salvato in 'best_models/best_coop_2-shot.pth' con accuracy 89.51%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e665437e9c654961b1260219da9cbf36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100, Loss: 0.0146\n",
      "Validation Accuracy: 89.51%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c99874aa43e41b19460f2e0c83163c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100, Loss: 0.0134\n",
      "Validation Accuracy: 89.80%\n",
      " Nuovo modello migliore salvato in 'best_models/best_coop_2-shot.pth' con accuracy 89.80%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43cf563d3d9040a58fe3a5971daca021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100, Loss: 0.0127\n",
      "Validation Accuracy: 89.80%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2976b27a50e147bcb65f08c476580b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100, Loss: 0.0125\n",
      "Validation Accuracy: 89.80%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4acef7f2b87f4c16ae90bed5bbee5006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100, Loss: 0.0124\n",
      "Validation Accuracy: 89.80%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a0d03f309c4c46ba811df98766c408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy CoOp (2-shot) sul TEST SET: 88.24%\n",
      "Differenza vs Zero-Shot: +16.95%\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Addestramento CoOp con 4 shot per classe ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bcf19159c79448d83bdaa9c4e11d4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Addestramento CoOp (4-shot):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368ae415c7d243fbae3cdfebd4f81e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 0.1699\n",
      "Validation Accuracy: 88.43%\n",
      " Nuovo modello migliore salvato in 'best_models/best_coop_4-shot.pth' con accuracy 88.43%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700f9bcb193d48768698323265c1eb26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100, Loss: 0.0482\n",
      "Validation Accuracy: 92.06%\n",
      " Nuovo modello migliore salvato in 'best_models/best_coop_4-shot.pth' con accuracy 92.06%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c2841a578a4e5ca6dbca903efe1f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100, Loss: 0.0284\n",
      "Validation Accuracy: 92.25%\n",
      " Nuovo modello migliore salvato in 'best_models/best_coop_4-shot.pth' con accuracy 92.25%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e88883f71a246c680ca9ff0c18a94cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100, Loss: 0.0210\n",
      "Validation Accuracy: 92.65%\n",
      " Nuovo modello migliore salvato in 'best_models/best_coop_4-shot.pth' con accuracy 92.65%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b22cb07ab6429aaf56e2cd1f3a85aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100, Loss: 0.0174\n",
      "Validation Accuracy: 92.65%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5d87d3a14b425c9e2a0e5fc99b931d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100, Loss: 0.0154\n",
      "Validation Accuracy: 92.84%\n",
      " Nuovo modello migliore salvato in 'best_models/best_coop_4-shot.pth' con accuracy 92.84%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d31451cd51664b3e8969bd71ba209b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100, Loss: 0.0141\n",
      "Validation Accuracy: 92.75%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7968aea43cd94da79350f1c324d663ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100, Loss: 0.0134\n",
      "Validation Accuracy: 92.65%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2bf096b303043d68d42885b3b94e6cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100, Loss: 0.0130\n",
      "Validation Accuracy: 92.65%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e123fdbb9fe4535a04b764d67014523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100, Loss: 0.0129\n",
      "Validation Accuracy: 92.65%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c178f94fda194d61b3b1eb824be595c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy CoOp (4-shot) sul TEST SET: 91.54%\n",
      "Differenza vs Zero-Shot: +20.25%\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Addestramento CoOp con 8 shot per classe ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59a34750eea4cb1af241468b0f97558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Addestramento CoOp (8-shot):   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68376b4ee3264ff6b4c82e2eac4d37ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200, Loss: 0.1420\n",
      "Validation Accuracy: 93.63%\n",
      " Nuovo modello migliore salvato in 'best_models/best_coop_8-shot.pth' con accuracy 93.63%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d3951b274f4bec845f3a68ba592124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/200, Loss: 0.0476\n",
      "Validation Accuracy: 94.31%\n",
      " Nuovo modello migliore salvato in 'best_models/best_coop_8-shot.pth' con accuracy 94.31%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4c38f06a9e4ccfb506a9943f166f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200, Loss: 0.0290\n",
      "Validation Accuracy: 94.61%\n",
      " Nuovo modello migliore salvato in 'best_models/best_coop_8-shot.pth' con accuracy 94.61%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17eb75b6ea3c482eb78bdf0ab33552ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/200, Loss: 0.0210\n",
      "Validation Accuracy: 94.31%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45db922bb85445afbc271f247adc0269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/200, Loss: 0.0168\n",
      "Validation Accuracy: 94.51%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e5e799f2e484a8a8c16beb15db6c3b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/200, Loss: 0.0142\n",
      "Validation Accuracy: 94.61%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1314851ce8fa4657bb764997b4f0634d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/200, Loss: 0.0122\n",
      "Validation Accuracy: 94.71%\n",
      " Nuovo modello migliore salvato in 'best_models/best_coop_8-shot.pth' con accuracy 94.71%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6098e606dcb8459495eb12e726f790dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/200, Loss: 0.0108\n",
      "Validation Accuracy: 94.61%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5a2c3d07f146dab38ec5c7eb31c1cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/200, Loss: 0.0099\n",
      "Validation Accuracy: 94.61%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817cd01d4ec6478ba0005a0087df92d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/200, Loss: 0.0092\n",
      "Validation Accuracy: 94.71%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e6178cc8bf4c799fb1a3ef61bed5f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/200, Loss: 0.0086\n",
      "Validation Accuracy: 94.71%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2173715d8b429183231656f2745a07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200, Loss: 0.0081\n",
      "Validation Accuracy: 94.90%\n",
      " Nuovo modello migliore salvato in 'best_models/best_coop_8-shot.pth' con accuracy 94.90%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e75866c0bd846f4b2129de503690bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/200, Loss: 0.0077\n",
      "Validation Accuracy: 94.90%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3554abefa27e42608f5132621c7f8988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/200, Loss: 0.0075\n",
      "Validation Accuracy: 95.00%\n",
      " Nuovo modello migliore salvato in 'best_models/best_coop_8-shot.pth' con accuracy 95.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed002a6c618141528d25ac9490dbb903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/200, Loss: 0.0073\n",
      "Validation Accuracy: 95.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f145e18bf124fbeb7086a81c615b2fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/200, Loss: 0.0071\n",
      "Validation Accuracy: 94.90%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c07a218c4bc4be39cee5ff39ab23dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/200, Loss: 0.0070\n",
      "Validation Accuracy: 95.10%\n",
      " Nuovo modello migliore salvato in 'best_models/best_coop_8-shot.pth' con accuracy 95.10%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417ae9683eac40b59ade979b079f5128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/200, Loss: 0.0069\n",
      "Validation Accuracy: 95.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1de0fd4850a4bd3ae2652c75c20eb10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190/200, Loss: 0.0069\n",
      "Validation Accuracy: 95.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12825e73b6e42b387f1c495476283ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/200, Loss: 0.0069\n",
      "Validation Accuracy: 95.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9639942ad66644dfbe742031f7ea1e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy CoOp (8-shot) sul TEST SET: 94.86%\n",
      "Differenza vs Zero-Shot: +23.56%\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Addestramento CoOp con 16 shot per classe ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cef65d95d264507a536f687f05f2e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Addestramento CoOp (16-shot):   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b8d431fed04829837b5fc1d98e44b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200, Loss: 0.1321\n",
      "Validation Accuracy: 93.43%\n",
      " Nuovo modello migliore salvato in 'best_models/best_coop_16-shot.pth' con accuracy 93.43%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c844393459964a768795a1e11776ee8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/200, Loss: 0.0471\n",
      "Validation Accuracy: 95.20%\n",
      " Nuovo modello migliore salvato in 'best_models/best_coop_16-shot.pth' con accuracy 95.20%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf5439d5c7ac40d7a9a83409680372da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200, Loss: 0.0296\n",
      "Validation Accuracy: 95.59%\n",
      " Nuovo modello migliore salvato in 'best_models/best_coop_16-shot.pth' con accuracy 95.59%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412c614c7e914470a30769647e347404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/200, Loss: 0.0216\n",
      "Validation Accuracy: 95.69%\n",
      " Nuovo modello migliore salvato in 'best_models/best_coop_16-shot.pth' con accuracy 95.69%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a41bf78b3348659324d0e3c008ee96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/200, Loss: 0.0172\n",
      "Validation Accuracy: 95.88%\n",
      " Nuovo modello migliore salvato in 'best_models/best_coop_16-shot.pth' con accuracy 95.88%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54f621d0d93493985e02762374751f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/200, Loss: 0.0144\n",
      "Validation Accuracy: 95.98%\n",
      " Nuovo modello migliore salvato in 'best_models/best_coop_16-shot.pth' con accuracy 95.98%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "016fcd2e353741e4913a83c290267af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/200, Loss: 0.0124\n",
      "Validation Accuracy: 96.18%\n",
      " Nuovo modello migliore salvato in 'best_models/best_coop_16-shot.pth' con accuracy 96.18%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf540fe1ddcc4d9da1c7cf45c5559518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/200, Loss: 0.0112\n",
      "Validation Accuracy: 95.88%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09dbbc47f61a456ab18c0210783647a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/200, Loss: 0.0102\n",
      "Validation Accuracy: 96.18%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e0c44eb410493b8abed363905d4ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/200, Loss: 0.0095\n",
      "Validation Accuracy: 96.57%\n",
      " Nuovo modello migliore salvato in 'best_models/best_coop_16-shot.pth' con accuracy 96.57%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6d3730cf0040df818b7d9650102f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/200, Loss: 0.0089\n",
      "Validation Accuracy: 96.18%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d557928016e344689ec809371ee58bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200, Loss: 0.0084\n",
      "Validation Accuracy: 96.47%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ef6bd97d50a4e6581614332b7f9227f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/200, Loss: 0.0080\n",
      "Validation Accuracy: 96.37%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb355c72c2f4b0b9cca5907991ddc31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/200, Loss: 0.0077\n",
      "Validation Accuracy: 96.27%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c530533870514f58a3943f08c2cb2cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/200, Loss: 0.0075\n",
      "Validation Accuracy: 96.37%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca290d8ae0d74e00b4b6a838b771551f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/200, Loss: 0.0074\n",
      "Validation Accuracy: 96.27%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702f5996ad9248bfb11400baabe345a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/200, Loss: 0.0073\n",
      "Validation Accuracy: 96.27%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a95392ea8224e2db4b27f8f3f994615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/200, Loss: 0.0072\n",
      "Validation Accuracy: 96.27%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c8a74df0f14a86ab0c102adc890078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190/200, Loss: 0.0071\n",
      "Validation Accuracy: 96.37%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627c5e60f22640b5aa95a77a88d2d58f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/200, Loss: 0.0071\n",
      "Validation Accuracy: 96.37%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b35fc15cb649569ad4b665f8e795c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy CoOp (16-shot) sul TEST SET: 95.40%\n",
      "Differenza vs Zero-Shot: +24.10%\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import InterpolationMode, Resize, CenterCrop, ToTensor, Normalize, Compose\n",
    "from torchvision.datasets import Flowers102\n",
    "import os\n",
    "random.seed(10)\n",
    "SHOTS = [1, 2, 4, 8, 16]\n",
    "M_CONTEXT_VECTORS = 16\n",
    "EPOCHS = [50,100,100,200,200] \n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 2e-3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_NAME = \"openai/clip-vit-base-patch16\"\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
    "if not os.path.exists(\"best_models\"):\n",
    "    os.makedirs(\"best_models\")\n",
    "\n",
    "\n",
    "#presa da https://github.com/openai/CLIP/blob/dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1/clip/clip.py#L79      \n",
    "def _convert_image_to_rgb(image):\n",
    "    return image.convert(\"RGB\")\n",
    "preprocess = Compose([\n",
    "        Resize(224, interpolation= InterpolationMode.BICUBIC),\n",
    "        CenterCrop(224),\n",
    "        _convert_image_to_rgb,\n",
    "        ToTensor(),\n",
    "        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "full_train_dataset = Flowers102(root=\"data\", split=\"train\", download=True, transform=preprocess)\n",
    "test_dataset = Flowers102(root=\"data\", split=\"test\", download=True, transform=preprocess)\n",
    "validation_dataset = Flowers102(root=\"data\", split=\"val\", download=True, transform=preprocess)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, pin_memory=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False, pin_memory=True)\n",
    "\n",
    "base_clip_model = CLIPModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "\n",
    "print(f\"Caricato il dataset Flowers102 con {len(full_train_dataset)} immagini di addestramento, {len(test_dataset)} immagini di test e {len(validation_dataset)} immagini di validazione.\")\n",
    "\n",
    "class_names = full_train_dataset.classes\n",
    "\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "    scheduler.step()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n--- Calcolo della Baseline Zero-Shot CLIP ---\")\n",
    "zeroshot_accuracy = evaluate(base_clip_model, test_loader, processor, class_names, is_zeroshot=True)\n",
    "#zeroshot_accuracy =13\n",
    "print(f\"Accuracy Zero-Shot CLIP: {zeroshot_accuracy:.2f}%\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "results = {}    \n",
    "for i, shot in enumerate(SHOTS):\n",
    "    print(f\"\\n--- Addestramento CoOp con {shot} shot per classe ---\")\n",
    "    train_dataset_few_shot = create_few_shot_split(full_train_dataset, shot)\n",
    "    train_loader_few_shot = DataLoader(train_dataset_few_shot, batch_size=BATCH_SIZE, shuffle=True , pin_memory=True)\n",
    "\n",
    "    \n",
    "    coop_model = CoOpCLIP(class_names, base_clip_model, m_ctx=M_CONTEXT_VECTORS).to(DEVICE)\n",
    "    \n",
    "    optimizer = torch.optim.SGD(coop_model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS[i])\n",
    "    \n",
    "    \n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_path = f\"best_models/best_coop_{shot}-shot.pth\"\n",
    "    \n",
    "    \n",
    "    pbar = tqdm(range(EPOCHS[i]), desc=f\"Addestramento CoOp ({shot}-shot)\")\n",
    "    \n",
    "    for epoch in pbar:\n",
    "        train_loss = train_one_epoch(coop_model, train_loader_few_shot, optimizer, scheduler)\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            coop_model.eval()\n",
    "            with torch.no_grad():\n",
    "                current_val_accuracy = evaluate(coop_model, validation_loader, processor, class_names)\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS[i]}, Loss: {train_loss:.4f}\")\n",
    "            print(f\"Validation Accuracy: {current_val_accuracy:.2f}%\")\n",
    "            if current_val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = current_val_accuracy\n",
    "                torch.save(coop_model.state_dict(), best_model_path)\n",
    "                print(f\" Nuovo modello migliore salvato in '{best_model_path}' con accuracy {best_val_accuracy:.2f}%\")\n",
    "            \n",
    "            coop_model.train()\n",
    "    \n",
    "    \n",
    "    \n",
    "    coop_accuracy = evaluate(coop_model, test_loader, processor, class_names)\n",
    "    results[shot] = coop_accuracy\n",
    "    \n",
    "    print(f\"Accuracy CoOp ({shot}-shot) sul TEST SET: {coop_accuracy:.2f}%\")\n",
    "    print(f\"Differenza vs Zero-Shot: {coop_accuracy - zeroshot_accuracy:+.2f}%\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "138ac8e706bd89d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T16:35:12.844571100Z",
     "start_time": "2025-07-02T16:28:43.175445700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dccc1f8847874f5d918f980bd2dd2510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== RIEPILOGO DEI RISULTATI =====\n",
      "Baseline Zero-Shot CLIP: 71.30%\n",
      "-----------------------------------\n",
      "Shots\t| CoOp Accuracy\t| Miglioramento\n",
      "-----------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a78d2f4802342bd8f69b1168c9291d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t| 79.93%\t\t| +8.64%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e8c2b55e0d4cde9565e2aedfa8dc99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\t| 88.13%\t\t| +16.83%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02f20039f9f4fe2adb3b49a72dbf1b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\t| 91.51%\t\t| +20.21%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc85e09a56a94ae19b521e181af23fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\t| 94.84%\t\t| +23.55%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0094486d22ed46f59079dfc6cb9a8d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valutazione:   0%|          | 0/193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\t| 95.35%\t\t| +24.05%\n",
      "===================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import Flowers102\n",
    "from transformers import  CLIPModel, CLIPProcessor\n",
    "from torchvision.transforms import InterpolationMode, Resize, CenterCrop, ToTensor, Normalize, Compose\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "M_CONTEXT_VECTORS = 16\n",
    "SHOTS = [1, 2, 4, 8, 16]\n",
    "def _convert_image_to_rgb(image):\n",
    "    return image.convert(\"RGB\")\n",
    "preprocess = Compose([\n",
    "    Resize(224, interpolation=InterpolationMode.BICUBIC),\n",
    "    CenterCrop(224),\n",
    "    _convert_image_to_rgb,\n",
    "    ToTensor(),\n",
    "    Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
    "full_train_dataset = Flowers102(root=\"data\", split=\"train\", download=True, transform=preprocess)\n",
    "test_dataset = Flowers102(root=\"data\", split=\"test\", download=True, transform=preprocess)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, pin_memory=True)\n",
    "class_names = full_train_dataset.classes\n",
    "base_clip_model = CLIPModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "zeroshot_accuracy = evaluate(base_clip_model, test_loader, processor, class_names, is_zeroshot=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n===== RIEPILOGO DEI RISULTATI =====\")\n",
    "print(f\"Baseline Zero-Shot CLIP: {zeroshot_accuracy:.2f}%\") \n",
    "print(\"-\" * 35)\n",
    "print(\"Shots\\t| CoOp Accuracy\\t| Miglioramento\")\n",
    "print(\"-\" * 35)\n",
    "results = {}\n",
    "for shot in SHOTS:\n",
    "    m = CoOpCLIP(class_names, base_clip_model, m_ctx=M_CONTEXT_VECTORS)\n",
    "    m.load_state_dict(torch.load(f\"best_models/best_coop_{shot}-shot.pth\"))\n",
    "    m.to(DEVICE)\n",
    "    m.eval()\n",
    "    with torch.no_grad():\n",
    "        acc = evaluate(m, test_loader, processor, class_names)\n",
    "    improvement = acc - zeroshot_accuracy\n",
    "    results[shot] = acc\n",
    "    print(f\"{shot}\\t| {acc:.2f}%\\t\\t| {improvement:+.2f}%\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "\n",
    "\n",
    "shots =  sorted(results.keys())\n",
    "accuracies =  [results[shot] for shot in sorted(results.keys())]\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "plt.plot(shots, accuracies, marker='o', linestyle='-', color='blue', label='CoOp Accuracy')\n",
    "plt.scatter(0, zeroshot_accuracy, color='red', s=100, label='Zero-Shot CLIP', zorder=5)\n",
    "\n",
    "plt.title(\"Accuratezza in funzione degli Shot\")\n",
    "plt.xlabel(\"Numero di Shot\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28ea3ab-1a4d-4d6f-abde-5412f5cdf3b8",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "The experimental results align well with the findings of the original CoOp paper, confirming that learning a soft prompt is an effective strategy for adapting CLIP to new tasks.\n",
    "\n",
    "A key observation from this experiment is the strong performance of the base CLIP model used. The initial zero-shot accuracy was 71.30%, which is noticeably higher than the 68.7% reported in the paper's baseline. This provided a higher starting point for our few-shot experiments.\n",
    "\n",
    "Intresting is the  performance advantage was maintained and built upon during the CoOp training stages. With just one training example per class, accuracy increased to 77.52%, and this positive trend continued as more data was introduced, reaching a final accuracy of 95.35% in the 8-shot scenario.\n",
    "\n",
    "In conclusion, the final results can be attributed to an effective combination: the experiment utilized a capable base CLIP model, and the CoOp training procedure successfully leveraged and extended this initial performance across all few-shot tests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
